#!/usr/bin/env python3
"""
üßπ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–†–û–¶–ï–°–°–û–† –û–ß–ò–°–¢–ö–ò –ë–ê–ó–´
–£–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ LLM —Ç–æ–∫–µ–Ω–æ–≤
"""

import os
import re
import json
import asyncio
from typing import Dict, List, Tuple, Optional
from notion_client import AsyncClient
from datetime import datetime

class AutoCleanupProcessor:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –±–∞–∑—ã"""
    
    def __init__(self):
        self.notion = AsyncClient(auth=os.getenv("NOTION_TOKEN"))
        self.database_id = "ad92a6e21485428c84de8587706b3be1"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        self.analysis_data = {}
        try:
            with open("telegram_full_analysis.json", "r", encoding="utf-8") as f:
                analysis_list = json.load(f)
                self.analysis_data = {item["page_id"]: item for item in analysis_list}
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.analysis_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        except FileNotFoundError:
            print("‚ùå –§–∞–π–ª telegram_full_analysis.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.stats = {
            'processed': 0,
            'titles_cleaned': 0,
            'descriptions_cleaned': 0,
            'tags_added': 0,
            'garbage_removed': 0,
            'links_processed': 0,
            'files_categorized': 0
        }

    def clean_title(self, title: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç –º—É—Å–æ—Ä–∞"""
        if not title:
            return title
            
        # –£–¥–∞–ª—è–µ–º —ç–º–æ–¥–∑–∏ –≤ –Ω–∞—á–∞–ª–µ
        title = re.sub(r'^[üì±üìÅüéØüî•‚ö°üßπüè∑Ô∏èüíéüöÄ]+\s*', '', title)
        
        # –£–¥–∞–ª—è–µ–º SaveAsBot —Å–ø–∞–º
        title = re.sub(r'–°–ø–∞—Å–∏–±–æ, —á—Ç–æ –ø–æ–ª—å–∑—É–µ—Ç–µ—Å—å.*?@SaveAsBot.*?\n*', '', title, flags=re.DOTALL)
        
        # –£–¥–∞–ª—è–µ–º —Ö—ç—à–∏ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        title = re.sub(r'[a-f0-9]{32,}', '', title)
        
        # –û—á–∏—â–∞–µ–º "üìÅ –§–∞–π–ª—ã (N):" –≤ –Ω–∞—á–∞–ª–µ
        title = re.sub(r'^üìÅ\s*–§–∞–π–ª—ã\s*\(\d+\):\s*', '', title)
        
        # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏–π
        if 'https://' in title and len(title) > 100:
            # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Å—ã–ª–∫–∏, –±–µ—Ä–µ–º —á–∞—Å—Ç—å –¥–æ —Å—Å—ã–ª–∫–∏
            before_link = title.split('https://')[0].strip()
            if len(before_link) > 10:
                title = before_link
        
        # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        if len(title) > 100:
            title = title[:97] + "..."
        
        return title.strip()

    def clean_description(self, description: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –æ—Ç –º—É—Å–æ—Ä–∞"""
        if not description:
            return description
            
        # –£–¥–∞–ª—è–µ–º SaveAsBot —Å–ø–∞–º
        description = re.sub(r'.*@SaveAsBot.*?\n?', '', description, flags=re.IGNORECASE)
        description = re.sub(r'–°–ø–∞—Å–∏–±–æ, —á—Ç–æ –ø–æ–ª—å–∑—É–µ—Ç–µ—Å—å.*?–æ–º\s*', '', description)
        
        # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–ø–∏—Å–∫–∏ —Ñ–∞–π–ª–æ–≤ –µ—Å–ª–∏ –æ–Ω–∏ –¥—É–±–ª–∏—Ä—É—é—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ
        if description.startswith('üìÅ –§–∞–π–ª—ã'):
            lines = description.split('\n')
            clean_lines = []
            for line in lines:
                if not (line.startswith('‚Ä¢') and ('.jpg' in line or '.mp4' in line or '.pdf' in line)):
                    clean_lines.append(line)
            description = '\n'.join(clean_lines)
        
        # –£–¥–∞–ª—è–µ–º —Ö—ç—à–∏
        description = re.sub(r'[a-f0-9]{32,}', '', description)
        
        return description.strip()

    def extract_smart_tags(self, title: str, description: str, links: List[str]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–º–Ω—ã–µ —Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        tags = []
        content = f"{title} {description}".lower()
        
        # –¢–µ–≥–∏ –ø–æ –¥–æ–º–µ–Ω–∞–º
        for link in links:
            if 'instagram.com' in link:
                tags.extend(['Instagram', '–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏'])
                if 'reel' in link:
                    tags.append('Reels')
            elif 'youtube.com' in link or 'youtu.be' in link:
                tags.extend(['YouTube', '–í–∏–¥–µ–æ'])
            elif 'github.com' in link:
                tags.extend(['GitHub', '–ö–æ–¥'])
            elif 'figma.com' in link:
                tags.extend(['Figma', '–î–∏–∑–∞–π–Ω'])
            elif 'yadi.sk' in link or 'yandex' in link:
                tags.append('–Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫')
        
        # –¢–µ–≥–∏ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
        if any(word in content for word in ['–¥–∏–∑–∞–π–Ω', 'ui', 'ux', 'figma', 'design']):
            tags.append('–î–∏–∑–∞–π–Ω')
        
        if any(word in content for word in ['–∫–æ–¥', '–ø—Ä–æ–≥—Ä–∞–º–º', 'python', 'javascript', 'github']):
            tags.append('–ö–æ–¥')
        
        if any(word in content for word in ['–≤–∏–¥–µ–æ', 'youtube', '—Å–º–æ—Ç—Ä–µ—Ç—å', '—Ñ–∏–ª—å–º']):
            tags.append('–í–∏–¥–µ–æ')
        
        if any(word in content for word in ['—Ñ–æ—Ç–æ', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω', '–∫–∞—Ä—Ç–∏–Ω–∫', 'photo', 'image']):
            tags.append('–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è')
        
        if any(word in content for word in ['–∞—É–¥–∏–æ', '–º—É–∑—ã–∫', '–∑–≤—É–∫', '–≥–æ–ª–æ—Å', 'mp3']):
            tags.append('–ê—É–¥–∏–æ')
        
        if any(word in content for word in ['–∏–¥–µ—è', '–º—ã—Å–ª—å', '–∫–æ–Ω—Ü–µ–ø—Ç', '–ø–ª–∞–Ω']):
            tags.append('–ò–¥–µ–∏')
        
        if any(word in content for word in ['–±–∏–∑–Ω–µ—Å', '–¥–µ–Ω—å–≥–∏', '–ø—Ä–æ–¥–∞–∂', '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥']):
            tags.append('–ë–∏–∑–Ω–µ—Å')
        
        if any(word in content for word in ['–æ–±—É—á–µ–Ω', '—É—Ä–æ–∫', '–∫—É—Ä—Å', 'learn']):
            tags.append('–û–±—É—á–µ–Ω–∏–µ')
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
        return list(set(tags))

    def is_garbage(self, analysis: Dict) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø–∏—Å—å –º—É—Å–æ—Ä–æ–º"""
        title = analysis['current_title']
        description = analysis['current_description']
        
        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        if len(title.strip()) < 3:
            return True
        
        # –¢–æ–ª—å–∫–æ —Ö—ç—à–∏
        if re.match(r'^[a-f0-9\s\-_]+$', title):
            return True
        
        # –¢–æ–ª—å–∫–æ SaveAsBot —Å–ø–∞–º
        if 'SaveAsBot' in title and len(title.replace('SaveAsBot', '').strip()) < 10:
            return True
        
        # –ë–∏—Ç—ã–µ —Å–∏–º–≤–æ–ª—ã
        if title.count('ÔøΩ') > 2:
            return True
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
        if any(word in title.lower() for word in ['test', '—Ç–µ—Å—Ç', '–ø—Ä–æ–≤–µ—Ä–∫–∞']):
            return True
        
        return False

    async def process_single_record(self, page_id: str, analysis: Dict) -> bool:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∑–∞–ø–∏—Å—å"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º—É—Å–æ—Ä
            if self.is_garbage(analysis):
                print(f"üóëÔ∏è –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä: {page_id}")
                await self.notion.pages.update(
                    page_id=page_id,
                    archived=True
                )
                self.stats['garbage_removed'] += 1
                return True
            
            # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            original_title = analysis['current_title']
            original_desc = analysis['current_description']
            original_tags = analysis['current_tags']
            
            new_title = self.clean_title(original_title)
            new_desc = self.clean_description(original_desc)
            new_tags = self.extract_smart_tags(
                new_title, 
                new_desc, 
                analysis.get('extracted_links', [])
            )
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏ –Ω–æ–≤—ã–µ —Ç–µ–≥–∏
            all_tags = list(set(original_tags + new_tags))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω—ã –ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            needs_update = False
            properties = {}
            
            if new_title != original_title and new_title:
                properties['Name'] = {
                    'title': [{'text': {'content': new_title}}]
                }
                self.stats['titles_cleaned'] += 1
                needs_update = True
            
            if new_desc != original_desc and new_desc:
                properties['–û–ø–∏—Å–∞–Ω–∏–µ'] = {
                    'rich_text': [{'text': {'content': new_desc}}]
                }
                self.stats['descriptions_cleaned'] += 1
                needs_update = True
            
            if len(all_tags) > len(original_tags):
                properties['–¢–µ–≥–∏'] = {
                    'multi_select': [{'name': tag} for tag in all_tags]
                }
                self.stats['tags_added'] += len(all_tags) - len(original_tags)
                needs_update = True
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
            if needs_update:
                await self.notion.pages.update(
                    page_id=page_id,
                    properties=properties
                )
                print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {page_id[:8]}... ({len(properties)} –∏–∑–º–µ–Ω–µ–Ω–∏–π)")
            
            self.stats['processed'] += 1
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {page_id}: {e}")
            return False

    async def process_all_records(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∑–∞–ø–∏—Å–∏"""
        print("üßπ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ß–ò–°–¢–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•")
        print("="*60)
        print(f"üìä –ó–∞–ø–∏—Å–µ–π –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(self.analysis_data)}")
        print()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø–∏—Å–∏ –±–∞—Ç—á–∞–º–∏
        batch_size = 10
        total_records = len(self.analysis_data)
        processed = 0
        
        for i in range(0, total_records, batch_size):
            batch_items = list(self.analysis_data.items())[i:i+batch_size]
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
            tasks = []
            for page_id, analysis in batch_items:
                task = self.process_single_record(page_id, analysis)
                tasks.append(task)
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –±–∞—Ç—á–∞
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            processed += len(batch_items)
            progress = (processed / total_records) * 100
            
            print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed}/{total_records} ({progress:.1f}%)")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            await asyncio.sleep(0.5)
        
        # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.print_final_stats()

    def print_final_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        print("\n" + "="*60)
        print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ß–ò–°–¢–ö–ò")
        print("="*60)
        print(f"üìÑ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['processed']}")
        print(f"üßπ –ù–∞–∑–≤–∞–Ω–∏–π –æ—á–∏—â–µ–Ω–æ: {self.stats['titles_cleaned']}")
        print(f"üìù –û–ø–∏—Å–∞–Ω–∏–π –æ—á–∏—â–µ–Ω–æ: {self.stats['descriptions_cleaned']}")
        print(f"üè∑Ô∏è –¢–µ–≥–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {self.stats['tags_added']}")
        print(f"üóëÔ∏è –ú—É—Å–æ—Ä–∞ —É–¥–∞–ª–µ–Ω–æ: {self.stats['garbage_removed']}")
        print()
        print("‚úÖ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ß–ò–°–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_data = {
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats
        }
        
        with open('auto_cleanup_stats.json', 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, ensure_ascii=False, indent=2)
        
        print("üíæ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ auto_cleanup_stats.json")

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    processor = AutoCleanupProcessor()
    if processor.analysis_data:
        await processor.process_all_records()
    else:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

if __name__ == "__main__":
    asyncio.run(main()) 