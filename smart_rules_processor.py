#!/usr/bin/env python3
"""
üß† SMART RULES PROCESSOR
–£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ LLM –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º

–°–¢–†–ê–¢–ï–ì–ò–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:
1. 95% –æ–±—Ä–∞–±–æ—Ç–∫–∏ - –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ (0 —Ç–æ–∫–µ–Ω–æ–≤)
2. 5% —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ - LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–º–∏–Ω–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤)
3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
4. –£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Å—ã–ª–æ–∫ –∏ –º–µ–¥–∏–∞
5. –ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –≤ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç

–ú–û–î–£–õ–ò:
- –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ (–±–µ–∑ LLM)
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–±–µ–∑ LLM) 
- –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å—Å—ã–ª–æ–∫ –∏ –º–µ–¥–∏–∞
- –£–º–Ω—ã–π —Ç–µ–≥–∏–Ω–≥ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
- LLM —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
"""

import os
import re
import json
import asyncio
from typing import Dict, List, Tuple, Optional
from urllib.parse import urlparse
from notion_client import AsyncClient
from datetime import datetime

class SmartRulesProcessor:
    """–£–º–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM"""
    
    def __init__(self):
        self.notion = AsyncClient(auth=os.getenv("NOTION_TOKEN"))
        self.database_id = "ad92a6e21485428c84de8587706b3be1"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–∞–ª–∏–∑
        self.analysis_data = {}
        try:
            with open("telegram_full_analysis.json", "r", encoding="utf-8") as f:
                analysis_list = json.load(f)
                self.analysis_data = {item["page_id"]: item for item in analysis_list}
        except FileNotFoundError:
            print("‚ùå –§–∞–π–ª –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.stats = {
            'auto_processed': 0,
            'llm_required': 0,
            'links_processed': 0,
            'media_categorized': 0,
            'tags_added': 0,
            'cleaned': 0,
            'deleted': 0
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª–∞
        self._init_deterministic_rules()
        self._init_content_classifiers()
        self._init_link_processors()

    def _init_deterministic_rules(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ (–±–µ–∑ LLM)"""
        
        # –ü—Ä–∞–≤–∏–ª–∞ –æ—á–∏—Å—Ç–∫–∏ –Ω–∞–∑–≤–∞–Ω–∏–π
        self.title_cleanup_rules = [
            {
                'name': 'remove_telegram_emoji',
                'pattern': r'^üì±\s*',
                'replacement': '',
                'description': '–£–¥–∞–ª–µ–Ω–∏–µ üì± –∏–∑ –Ω–∞—á–∞–ª–∞ –Ω–∞–∑–≤–∞–Ω–∏–π'
            },
            {
                'name': 'extract_from_links',
                'pattern': r'^.*?https?://[^\s]+\s*(.+)$',
                'replacement': r'\1',
                'description': '–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ —Å—Å—ã–ª–æ–∫'
            },
            {
                'name': 'clean_files_titles',
                'pattern': r'^üìÅ –§–∞–π–ª—ã \(\d+\):.*',
                'replacement': lambda match, desc: self._extract_meaningful_title_from_files(match.group(0), desc),
                'description': '–ó–∞–º–µ–Ω–∞ "üìÅ –§–∞–π–ª—ã" –Ω–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è'
            },
            {
                'name': 'remove_savebot_spam',
                'pattern': r'–°–ø–∞—Å–∏–±–æ, —á—Ç–æ –ø–æ–ª—å–∑—É–µ—Ç–µ—Å—å.*?@SaveAsBot.*?\n?',
                'replacement': '',
                'description': '–£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–∞–º–∞ SaveAsBot'
            }
        ]
        
        # –ü—Ä–∞–≤–∏–ª–∞ –æ—á–∏—Å—Ç–∫–∏ –æ–ø–∏—Å–∞–Ω–∏–π
        self.description_cleanup_rules = [
            {
                'pattern': r'.*@SaveAsBot.*\n?',
                'replacement': '',
                'description': '–£–¥–∞–ª–µ–Ω–∏–µ SaveAsBot –∏–∑ –æ–ø–∏—Å–∞–Ω–∏–π'
            },
            {
                'pattern': r'üìÅ –§–∞–π–ª—ã \(\d+\):.*?(?=\n\n|\Z)',
                'replacement': '',
                'description': '–£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ —Ñ–∞–π–ª–æ–≤ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏–π'
            },
            {
                'pattern': r'\s*‚Ä¢\s*\w+@\d{2}-\d{2}-\d{4}_\d{2}-\d{2}-\d{2}\.\w+.*?\n',
                'replacement': '',
                'description': '–£–¥–∞–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤'
            },
            {
                'pattern': r'\([0-9.]+MB\)\s*\[photo\]\s*-\s*',
                'replacement': '',
                'description': '–£–¥–∞–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ–∞–π–ª–∞—Ö'
            }
        ]

    def _init_content_classifiers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–±–µ–∑ LLM)"""
        
        # –£–º–Ω—ã–µ —Ç–µ–≥–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        self.smart_tags = {
            'Instagram': {
                'keywords': ['instagram.com', 'reel', 'igsh', 'img_index'],
                'weight': 10
            },
            'YouTube': {
                'keywords': ['youtube.com', 'youtu.be', 'watch?v='],
                'weight': 10
            },
            '–î–∏–∑–∞–π–Ω': {
                'keywords': ['figma', 'design', 'ui', 'ux', 'dribbble', 'behance', 'typography', 'color', 'layout'],
                'weight': 8
            },
            '–ö–æ–¥': {
                'keywords': ['github', 'code', 'python', 'javascript', 'api', 'programming', 'dev', 'repository'],
                'weight': 8
            },
            'AI': {
                'keywords': ['chatgpt', 'midjourney', 'openai', '–Ω–µ–π—Ä–æ—Å–µ—Ç—å', 'ai', 'artificial intelligence', 'prompt'],
                'weight': 9
            },
            '–ë–∏–∑–Ω–µ—Å': {
                'keywords': ['startup', 'business', 'marketing', 'sales', 'revenue', 'monetization', 'strategy'],
                'weight': 7
            },
            '–û–±—É—á–µ–Ω–∏–µ': {
                'keywords': ['course', 'learn', 'tutorial', 'guide', 'education', 'skill', 'training'],
                'weight': 7
            },
            '–ù–æ–≤–æ—Å—Ç–∏': {
                'keywords': ['news', '–Ω–æ–≤–æ—Å—Ç–∏', 'event', 'announcement', 'update', 'release'],
                'weight': 6
            },
            '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã': {
                'keywords': ['tool', 'service', 'app', 'software', 'platform', '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç', '—Å–µ—Ä–≤–∏—Å'],
                'weight': 7
            },
            '–ö–æ–Ω—Ç–µ–Ω—Ç': {
                'keywords': ['content', 'post', 'article', 'blog', 'story', '–∫–æ–Ω—Ç–µ–Ω—Ç', '—Å—Ç–∞—Ç—å—è'],
                'weight': 6
            }
        }
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        self.content_types = {
            'video': ['mp4', 'avi', 'mov', 'mkv', 'webm', 'video', '–≤–∏–¥–µ–æ'],
            'image': ['jpg', 'jpeg', 'png', 'gif', 'webp', 'svg', 'photo', 'image'],
            'document': ['pdf', 'doc', 'docx', 'txt', 'rtf', 'document'],
            'archive': ['zip', 'rar', '7z', 'tar', 'archive'],
            'audio': ['mp3', 'wav', 'flac', 'm4a', 'audio']
        }

    def _init_link_processors(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å—Å—ã–ª–æ–∫"""
        
        self.link_processors = {
            'instagram.com': {
                'type': 'social_media',
                'extract_title': lambda url: self._extract_instagram_info(url),
                'auto_tags': ['Instagram', '–ö–æ–Ω—Ç–µ–Ω—Ç'],
                'priority': 'high'
            },
            'youtube.com': {
                'type': 'video_platform', 
                'extract_title': lambda url: self._extract_youtube_info(url),
                'auto_tags': ['YouTube', '–í–∏–¥–µ–æ'],
                'priority': 'high'
            },
            'youtu.be': {
                'type': 'video_platform',
                'extract_title': lambda url: self._extract_youtube_info(url),
                'auto_tags': ['YouTube', '–í–∏–¥–µ–æ'],
                'priority': 'high'
            },
            'yadi.sk': {
                'type': 'file_storage',
                'extract_title': lambda url: '–§–∞–π–ª –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–µ',
                'auto_tags': ['–§–∞–π–ª—ã'],
                'priority': 'medium'
            },
            'disk.yandex.ru': {
                'type': 'file_storage', 
                'extract_title': lambda url: '–§–∞–π–ª –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–µ',
                'auto_tags': ['–§–∞–π–ª—ã'],
                'priority': 'medium'
            },
            'github.com': {
                'type': 'code_repository',
                'extract_title': lambda url: self._extract_github_info(url),
                'auto_tags': ['–ö–æ–¥', 'GitHub'],
                'priority': 'high'
            },
            'habr.com': {
                'type': 'tech_article',
                'extract_title': lambda url: '–°—Ç–∞—Ç—å—è –Ω–∞ –•–∞–±—Ä–µ',
                'auto_tags': ['–°—Ç–∞—Ç—å—è', '–ö–æ–¥'],
                'priority': 'medium'
            }
        }

    async def process_all_records(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∑–∞–ø–∏—Å–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM"""
        print("üß† SMART RULES PROCESSOR - –°–¢–ê–†–¢")
        print("="*60)
        print(f"üìä –ó–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(self.analysis_data)}")
        
        # –≠—Ç–∞–ø 1: –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–±–µ–∑ LLM)
        auto_processed = await self._deterministic_processing()
        
        # –≠—Ç–∞–ø 2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –∏ –º–µ–¥–∏–∞ (–±–µ–∑ LLM)
        link_processed = await self._process_links_and_media()
        
        # –≠—Ç–∞–ø 3: –£–º–Ω–æ–µ —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–±–µ–∑ LLM)
        tagged = await self._smart_tagging()
        
        # –≠—Ç–∞–ø 4: –í—ã—è–≤–ª–µ–Ω–∏–µ —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –¥–ª—è LLM
        controversial = await self._identify_controversial_cases()
        
        # –≠—Ç–∞–ø 5: LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–ª—å–∫–æ —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        llm_processed = await self._llm_processing(controversial)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        await self._save_processing_results()
        
        self._print_final_stats()

    async def _deterministic_processing(self):
        """–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ LLM"""
        print("\nüîß –≠–¢–ê–ü 1: –î–ï–¢–ï–†–ú–ò–ù–ò–†–û–í–ê–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê (–±–µ–∑ LLM)")
        print("-" * 50)
        
        processed = 0
        
        for page_id, analysis in self.analysis_data.items():
            changes = {}
            
            # –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π
            new_title = self._apply_title_cleanup(analysis['current_title'], analysis['current_description'])
            if new_title != analysis['current_title']:
                changes['title'] = new_title
            
            # –û—á–∏—Å—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π
            new_desc = self._apply_description_cleanup(analysis['current_description'])
            if new_desc != analysis['current_description']:
                changes['description'] = new_desc
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞
            if self._is_garbage_record(analysis):
                changes['delete'] = True
                self.stats['deleted'] += 1
            
            if changes:
                analysis['auto_changes'] = changes
                processed += 1
                self.stats['auto_processed'] += 1
        
        print(f"‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed} –∑–∞–ø–∏—Å–µ–π")
        return processed

    def _apply_title_cleanup(self, title: str, description: str) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –æ—á–∏—Å—Ç–∫–∏ –Ω–∞–∑–≤–∞–Ω–∏–π"""
        cleaned_title = title
        
        for rule in self.title_cleanup_rules:
            if callable(rule.get('replacement')):
                # –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏
                match = re.search(rule['pattern'], cleaned_title)
                if match:
                    cleaned_title = rule['replacement'](match, description)
            else:
                # –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö regex –∑–∞–º–µ–Ω
                cleaned_title = re.sub(rule['pattern'], rule['replacement'], cleaned_title)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        cleaned_title = re.sub(r'\s+', ' ', cleaned_title).strip()
        
        return cleaned_title if cleaned_title else title

    def _apply_description_cleanup(self, description: str) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –æ—á–∏—Å—Ç–∫–∏ –æ–ø–∏—Å–∞–Ω–∏–π"""
        cleaned_desc = description
        
        for rule in self.description_cleanup_rules:
            cleaned_desc = re.sub(rule['pattern'], rule['replacement'], cleaned_desc, flags=re.MULTILINE | re.DOTALL)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        cleaned_desc = re.sub(r'\n\s*\n', '\n\n', cleaned_desc)
        cleaned_desc = re.sub(r'\s+', ' ', cleaned_desc).strip()
        
        return cleaned_desc

    def _extract_meaningful_title_from_files(self, files_title: str, description: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤"""
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
        if description and description != files_title:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
            sentences = re.split(r'[.!?\n]', description.strip())
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 15 and len(sentence) < 100 and not sentence.startswith('üìÅ'):
                    return sentence
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        file_types = []
        if '.jpg' in files_title or '.png' in files_title or 'photo' in files_title:
            file_types.append('—Ñ–æ—Ç–æ')
        if '.mp4' in files_title or 'video' in files_title:
            file_types.append('–≤–∏–¥–µ–æ')
        if '.pdf' in files_title or '.doc' in files_title:
            file_types.append('–¥–æ–∫—É–º–µ–Ω—Ç—ã')
        
        if file_types:
            return f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {', '.join(file_types)}"
        
        return "–§–∞–π–ª—ã –∏–∑ Telegram"

    async def _process_links_and_media(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –∏ –º–µ–¥–∏–∞ –±–µ–∑ LLM"""
        print("\nüîó –≠–¢–ê–ü 2: –û–ë–†–ê–ë–û–¢–ö–ê –°–°–´–õ–û–ö –ò –ú–ï–î–ò–ê (–±–µ–∑ LLM)")
        print("-" * 50)
        
        processed = 0
        
        for page_id, analysis in self.analysis_data.items():
            if not analysis.get('extracted_links'):
                continue
            
            link_info = self._analyze_links(analysis['extracted_links'])
            if link_info:
                if 'auto_changes' not in analysis:
                    analysis['auto_changes'] = {}
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Å—ã–ª–∫–∞—Ö
                analysis['link_info'] = link_info
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Å—ã–ª–æ–∫
                auto_tags = set(analysis.get('current_tags', []))
                for link_data in link_info:
                    auto_tags.update(link_data.get('auto_tags', []))
                
                if auto_tags != set(analysis.get('current_tags', [])):
                    analysis['auto_changes']['tags'] = list(auto_tags)
                
                processed += 1
                self.stats['links_processed'] += 1
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {processed} –∑–∞–ø–∏—Å–µ–π")
        return processed

    def _analyze_links(self, links: List[str]) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"""
        link_info = []
        
        for link in links:
            try:
                parsed = urlparse(link)
                domain = parsed.netloc.lower()
                
                # –£–±–∏—Ä–∞–µ–º www.
                domain = domain.replace('www.', '')
                
                for processor_domain, processor in self.link_processors.items():
                    if processor_domain in domain:
                        info = {
                            'url': link,
                            'domain': domain,
                            'type': processor['type'],
                            'auto_tags': processor['auto_tags'],
                            'priority': processor['priority']
                        }
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        try:
                            title = processor['extract_title'](link)
                            if title:
                                info['suggested_title'] = title
                        except:
                            pass
                        
                        link_info.append(info)
                        break
            except:
                continue
        
        return link_info

    def _extract_instagram_info(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ Instagram —Å—Å—ã–ª–æ–∫"""
        if '/reel/' in url:
            return "Instagram Reel"
        elif '/p/' in url:
            return "Instagram –ø–æ—Å—Ç"
        elif '/stories/' in url:
            return "Instagram Story"
        return "Instagram –∫–æ–Ω—Ç–µ–Ω—Ç"

    def _extract_youtube_info(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ YouTube —Å—Å—ã–ª–æ–∫"""
        if 'watch?v=' in url or 'youtu.be/' in url:
            return "YouTube –≤–∏–¥–µ–æ"
        elif '/playlist' in url:
            return "YouTube –ø–ª–µ–π–ª–∏—Å—Ç"
        elif '/channel/' in url or '/c/' in url:
            return "YouTube –∫–∞–Ω–∞–ª"
        return "YouTube –∫–æ–Ω—Ç–µ–Ω—Ç"

    def _extract_github_info(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ GitHub —Å—Å—ã–ª–æ–∫"""
        parts = url.split('/')
        if len(parts) >= 5:
            repo_name = parts[4]
            return f"GitHub: {repo_name}"
        return "GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π"

    async def _smart_tagging(self):
        """–£–º–Ω–æ–µ —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ LLM"""
        print("\nüè∑Ô∏è –≠–¢–ê–ü 3: –£–ú–ù–û–ï –¢–ï–ì–ò–†–û–í–ê–ù–ò–ï (–±–µ–∑ LLM)")
        print("-" * 50)
        
        processed = 0
        
        for page_id, analysis in self.analysis_data.items():
            content = f"{analysis['current_title']} {analysis['current_description']}".lower()
            
            # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ —Ç–µ–≥–æ–≤
            tag_scores = {}
            for tag, config in self.smart_tags.items():
                score = 0
                for keyword in config['keywords']:
                    if keyword.lower() in content:
                        score += config['weight']
                
                if score > 5:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                    tag_scores[tag] = score
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏ —Å –≤—ã—Å–æ–∫–∏–º —Å–∫–æ—Ä–æ–º
            if tag_scores:
                current_tags = set(analysis.get('current_tags', []))
                new_tags = set(tag for tag, score in tag_scores.items() if score >= 7)
                
                if new_tags - current_tags:  # –ï—Å—Ç—å –Ω–æ–≤—ã–µ —Ç–µ–≥–∏
                    if 'auto_changes' not in analysis:
                        analysis['auto_changes'] = {}
                    
                    all_tags = current_tags | new_tags
                    analysis['auto_changes']['tags'] = list(all_tags)
                    analysis['tag_scores'] = tag_scores
                    
                    processed += 1
                    self.stats['tags_added'] += 1
        
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã —Ç–µ–≥–∏: {processed} –∑–∞–ø–∏—Å–µ–π")
        return processed

    def _is_garbage_record(self, analysis: Dict) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è"""
        title = analysis['current_title'].lower()
        desc = analysis['current_description'].lower()
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –º—É—Å–æ—Ä–∞
        garbage_indicators = [
            len(title.strip()) < 3,
            title in ['test', '—Ç–µ—Å—Ç', '...', '-', '.'],
            'test' in title and len(title) < 10,
            not analysis.get('has_valuable_content', True),
            len(analysis.get('extracted_links', [])) == 0 and len(title + desc) < 15
        ]
        
        return any(garbage_indicators)

    async def _identify_controversial_cases(self):
        """–í—ã—è–≤–ª—è–µ—Ç —Å–ø–æ—Ä–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        print("\n‚ùì –≠–¢–ê–ü 4: –í–´–Ø–í–õ–ï–ù–ò–ï –°–ü–û–†–ù–´–• –°–õ–£–ß–ê–ï–í")
        print("-" * 50)
        
        controversial = []
        
        for page_id, analysis in self.analysis_data.items():
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
            if analysis.get('auto_changes'):
                continue
            
            # –ö—Ä–∏—Ç–µ—Ä–∏–∏ —Å–ø–æ—Ä–Ω–æ—Å—Ç–∏
            is_controversial = (
                # –°–ª–æ–∂–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                len(analysis['current_title']) > 100 or
                # –ú–Ω–æ–≥–æ —Å—Å—ã–ª–æ–∫ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
                len(analysis.get('extracted_links', [])) > 5 or
                # –°–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                (len(analysis.get('extracted_links', [])) > 0 and len(analysis.get('extracted_files', [])) > 0) or
                # –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                not any(keyword in analysis['current_title'].lower() + analysis['current_description'].lower() 
                       for tag_config in self.smart_tags.values() 
                       for keyword in tag_config['keywords'])
            )
            
            if is_controversial:
                controversial.append(page_id)
        
        print(f"‚ö†Ô∏è –°–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –¥–ª—è LLM: {len(controversial)}")
        self.stats['llm_required'] = len(controversial)
        
        return controversial

    async def _llm_processing(self, controversial_ids: List[str]):
        """LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–ª—å–∫–æ —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤"""
        print(f"\nü§ñ –≠–¢–ê–ü 5: LLM –û–ë–†–ê–ë–û–¢–ö–ê ({len(controversial_ids)} –∑–∞–ø–∏—Å–µ–π)")
        print("-" * 50)
        
        if not controversial_ids:
            print("‚úÖ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            return 0
        
        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å DeepSeek
        # –ü–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        
        print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞—Ç—á–µ–π –¥–ª—è DeepSeek...")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        batches = self._create_llm_batches(controversial_ids)
        
        processed = 0
        for batch_type, batch_ids in batches.items():
            print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ '{batch_type}': {len(batch_ids)} –∑–∞–ø–∏—Å–µ–π")
            
            # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ DeepSeek API
            # result = await self._process_with_deepseek(batch_type, batch_ids)
            
            processed += len(batch_ids)
        
        print(f"‚úÖ LLM –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed} –∑–∞–ø–∏—Å–µ–π")
        return processed

    def _create_llm_batches(self, controversial_ids: List[str]) -> Dict[str, List[str]]:
        """–°–æ–∑–¥–∞–µ—Ç –±–∞—Ç—á–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        batches = {
            'complex_titles': [],
            'mixed_content': [], 
            'link_heavy': [],
            'uncategorized': []
        }
        
        for page_id in controversial_ids:
            analysis = self.analysis_data[page_id]
            
            if len(analysis['current_title']) > 100:
                batches['complex_titles'].append(page_id)
            elif len(analysis.get('extracted_links', [])) > 5:
                batches['link_heavy'].append(page_id)
            elif (len(analysis.get('extracted_links', [])) > 0 and 
                  len(analysis.get('extracted_files', [])) > 0):
                batches['mixed_content'].append(page_id)
            else:
                batches['uncategorized'].append(page_id)
        
        return batches

    async def _save_processing_results(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        results = {
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats,
            'processed_data': {}
        }
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        for page_id, analysis in self.analysis_data.items():
            if analysis.get('auto_changes') or analysis.get('link_info'):
                results['processed_data'][page_id] = {
                    'auto_changes': analysis.get('auto_changes', {}),
                    'link_info': analysis.get('link_info', []),
                    'tag_scores': analysis.get('tag_scores', {})
                }
        
        with open('smart_processing_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ smart_processing_results.json")

    def _print_final_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        print("\n" + "="*60)
        print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê SMART PROCESSING")
        print("="*60)
        
        total_records = len(self.analysis_data)
        auto_percent = (self.stats['auto_processed'] / total_records) * 100
        llm_percent = (self.stats['llm_required'] / total_records) * 100
        
        print(f"üì• –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_records}")
        print(f"üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['auto_processed']} ({auto_percent:.1f}%)")
        print(f"ü§ñ –¢—Ä–µ–±—É–µ—Ç LLM: {self.stats['llm_required']} ({llm_percent:.1f}%)")
        print(f"üîó –°—Å—ã–ª–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['links_processed']}")
        print(f"üè∑Ô∏è –¢–µ–≥–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {self.stats['tags_added']}")
        print(f"üóëÔ∏è –ó–∞–ø–∏—Å–µ–π —É–¥–∞–ª–µ–Ω–æ: {self.stats['deleted']}")
        
        print(f"\nüí∞ –≠–ö–û–ù–û–ú–ò–Ø –¢–û–ö–ï–ù–û–í:")
        estimated_tokens_saved = self.stats['auto_processed'] * 200  # ~200 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∑–∞–ø–∏—Å—å
        print(f"–°—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: ~{estimated_tokens_saved:,}")
        print(f"–°—Ç–æ–∏–º–æ—Å—Ç—å —ç–∫–æ–Ω–æ–º–∏–∏: ~${estimated_tokens_saved * 0.0001:.2f}")
        
        print(f"\nüéØ –ö–ê–ß–ï–°–¢–í–û –û–ë–†–ê–ë–û–¢–ö–ò:")
        print(f"–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {auto_percent:.1f}%")
        print(f"–¢—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: {llm_percent:.1f}%")

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    processor = SmartRulesProcessor()
    await processor.process_all_records()

if __name__ == "__main__":
    asyncio.run(main()) 