#!/usr/bin/env python3
"""
üé§ ENHANCED VOICE TRAINING SYSTEM
–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥–∞—Ö —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º –º—É–¥—Ä–æ—Å—Ç–∏

–ù–û–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
1. –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –ø–æ ID –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é
2. –£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ URL (—Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã/—Å–æ—Ü—Å–µ—Ç–∏/SMM)
3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏
4. –û—á–∏—Å—Ç–∫–∞ –æ—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
5. –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –º—É–¥—Ä–æ—Å—Ç–∏ –≤ JSON
6. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ LLM
"""

import os
import re
import json
import asyncio
from typing import Dict, List, Tuple, Optional
from notion_client import AsyncClient
from datetime import datetime
import hashlib

class EnhancedVoiceTrainingSystem:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥–∞—Ö"""
    
    def __init__(self):
        self.notion = AsyncClient(auth=os.getenv("NOTION_TOKEN"))
        self.database_id = "ad92a6e21485428c84de8587706b3be1"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞
        self.analysis_data = {}
        try:
            with open("telegram_full_analysis.json", "r", encoding="utf-8") as f:
                analysis_list = json.load(f)
                self.analysis_data = {item["page_id"]: item for item in analysis_list}
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.analysis_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        except FileNotFoundError:
            print("‚ùå –§–∞–π–ª telegram_full_analysis.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –æ–±—É—á–µ–Ω–∏—è
        self.training_rules = []
        self.learned_patterns = []
        self.wisdom_base = self._load_wisdom_base()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ LLM –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        self.llm_processors = {
            'deepseek': self._init_deepseek_llm(),
            'claude': self._init_claude_llm(),
            'gpt4': self._init_gpt4_llm()
        }

    def _load_wisdom_base(self) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –º—É–¥—Ä–æ—Å—Ç–∏"""
        try:
            with open('wisdom_base.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {
                'patterns': [],
                'successful_rules': [],
                'failed_rules': [],
                'user_preferences': {},
                'domain_knowledge': {},
                'last_updated': datetime.now().isoformat()
            }

    def _save_wisdom_base(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –±–∞–∑—É –º—É–¥—Ä–æ—Å—Ç–∏"""
        self.wisdom_base['last_updated'] = datetime.now().isoformat()
        with open('wisdom_base.json', 'w', encoding='utf-8') as f:
            json.dump(self.wisdom_base, f, ensure_ascii=False, indent=2)

    def _init_deepseek_llm(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DeepSeek LLM"""
        try:
            # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å DeepSeek
            return {'available': True, 'model': 'deepseek-chat'}
        except:
            return {'available': False}

    def _init_claude_llm(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Claude LLM"""
        try:
            # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Claude
            return {'available': True, 'model': 'claude-3-sonnet'}
        except:
            return {'available': False}

    def _init_gpt4_llm(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPT-4 LLM"""
        try:
            # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å GPT-4
            return {'available': True, 'model': 'gpt-4'}
        except:
            return {'available': False}

    def start_enhanced_voice_learning(self):
        """–ù–∞—á–∏–Ω–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥–∞—Ö"""
        print("üé§ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ì–û–õ–û–°–û–í–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
        print("="*70)
        print("–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
        print("‚Ä¢ –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –ø–æ ID –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é")
        print("‚Ä¢ –£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ URL (—Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã/—Å–æ—Ü—Å–µ—Ç–∏/SMM)")
        print("‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏")
        print("‚Ä¢ –û—á–∏—Å—Ç–∫–∞ –æ—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
        print("‚Ä¢ –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –º—É–¥—Ä–æ—Å—Ç–∏ –≤ JSON")
        print("‚Ä¢ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ LLM")
        print()
        
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ —Ñ–∞–π–ª–∞
        file_rules = self._load_rules_from_file()
        if file_rules:
            print(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(file_rules)} –ø—Ä–∞–≤–∏–ª –∏–∑ —Ñ–∞–π–ª–∞ voice_training_rules.txt")
            self._parse_enhanced_voice_rules(file_rules)
        
        print("–ù–∞–≥–æ–≤–∞—Ä–∏–≤–∞–π—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∫–∏ –≤ —Å–≤–æ–±–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–µ:")
        print("–ü—Ä–∏–º–µ—Ä—ã:")
        print('- "–ó–∞–ø–∏—Å–∏ —Å ID 1104 —É–¥–∞–ª–∏—Ç—å - —É—Å—Ç–∞—Ä–µ–≤—à–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"')
        print('- "URL –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∞–π–ª–æ–≤, —Å–æ—Ü—Å–µ—Ç–∏ –∏ SMM"')
        print('- "–£–¥–∞–ª–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ Wildberries –µ—Å–ª–∏ –Ω–µ –ø–æ —Ç–µ–º–µ"')
        print('- "–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–≥ SMM –µ—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å–æ—Ü—Å–µ—Ç–µ–π"')
        print('- "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å 4 –¥–ª—è SMM –∏ —Å–æ—Ü—Å–µ—Ç–µ–π"')
        print()
        print("–ì–æ–≤–æ—Ä–∏—Ç–µ –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∞–≤–∏–ª—É –Ω–∞ —Å—Ç—Ä–æ–∫—É. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ.")
        print()
        
        voice_rules = []
        while True:
            rule = input("üé§ –ü—Ä–∞–≤–∏–ª–æ: ").strip()
            if not rule:
                break
            voice_rules.append(rule)
            print(f"‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ: {rule[:50]}...")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ —Ñ–∞–π–ª–∞ –∏ –≥–æ–ª–æ—Å–æ–≤—ã–µ
        all_rules = file_rules + voice_rules
        
        if all_rules:
            if voice_rules:  # –ï—Å–ª–∏ –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –Ω–æ–≤—ã–µ –≥–æ–ª–æ—Å–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞
                self._parse_enhanced_voice_rules(voice_rules)
            self._save_training_data()
            self._update_wisdom_base(all_rules)
            print(f"\nüß† –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –°–æ–∑–¥–∞–Ω–æ {len(self.learned_patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
            self._show_learned_patterns()
        else:
            print("‚ùå –ü—Ä–∞–≤–∏–ª–∞ –Ω–µ –≤–≤–µ–¥–µ–Ω—ã")

    def _load_rules_from_file(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ —Ñ–∞–π–ª–∞ voice_training_rules.txt"""
        try:
            with open('voice_training_rules.txt', 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ —Ñ–∞–π–ª–∞ (—Å—Ç—Ä–æ–∫–∏ –≤ –∫–∞–≤—ã—á–∫–∞—Ö)
            rules = []
            for line in content.split('\n'):
                line = line.strip()
                if line.startswith('- "') and line.endswith('"'):
                    rule = line[3:-1]  # —É–±–∏—Ä–∞–µ–º '- "' –∏ '"'
                    rules.append(rule)
                elif line.startswith('"') and line.endswith('"'):
                    rule = line[1:-1]  # —É–±–∏—Ä–∞–µ–º '"'
                    rules.append(rule)
            
            return rules
        except FileNotFoundError:
            print("‚ö†Ô∏è –§–∞–π–ª voice_training_rules.txt –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –ø—Ä–∞–≤–∏–ª: {e}")
            return []

    def _parse_enhanced_voice_rules(self, voice_rules: List[str]):
        """–ü–∞—Ä—Å–∏—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –≥–æ–ª–æ—Å–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞"""
        print(f"\nüß† –ê–ù–ê–õ–ò–ó {len(voice_rules)} –£–õ–£–ß–®–ï–ù–ù–´–• –ì–û–õ–û–°–û–í–´–• –ü–†–ê–í–ò–õ")
        print("-" * 50)
        
        for rule in voice_rules:
            pattern = self._extract_enhanced_pattern(rule)
            if pattern:
                self.learned_patterns.append(pattern)
                print(f"‚úÖ –ü–∞—Ç—Ç–µ—Ä–Ω: {pattern['type']} - {pattern['description']}")
            else:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å: {rule}")

    def _extract_enhanced_pattern(self, rule: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞"""
        rule_lower = rule.lower()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω —É–¥–∞–ª–µ–Ω–∏—è –∑–∞–ø–∏—Å–µ–π –ø–æ ID
        id_match = re.search(r'id\s+(\d+)', rule, re.IGNORECASE)
        if id_match and '—É–¥–∞–ª–∏—Ç—å' in rule_lower:
            page_id = id_match.group(1)
            return {
                'type': 'record_deletion',
                'action': 'delete_by_id',
                'description': f'–£–¥–∞–ª–∏—Ç—å –∑–∞–ø–∏—Å—å —Å ID {page_id}',
                'page_id': page_id,
                'reason': rule
            }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –æ—á–∏—Å—Ç–∫–∏ –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ç –±–∏–ª–∏–±–µ—Ä–¥—ã
        elif '–±–∏–ª–∏–±–µ—Ä–¥–∞' in rule_lower and ('–Ω–∞–∑–≤–∞–Ω–∏–µ' in rule_lower or 'id' in rule_lower):
            id_match = re.search(r'id\s+(\d+)', rule, re.IGNORECASE)
            if id_match:
                page_id = id_match.group(1)
                return {
                    'type': 'title_cleanup',
                    'action': 'clean_gibberish',
                    'description': f'–û—á–∏—Å—Ç–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ {page_id} –æ—Ç –±–∏–ª–∏–±–µ—Ä–¥—ã',
                    'page_id': page_id,
                    'patterns_to_remove': [
                        r'\s+',  # –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
                        r'i don\'t know what this is',
                        r'[^\w\s\-\.]',  # —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                        r'\b(om|ok|i\'ll|look|video|about)\b'  # –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞-–º—É—Å–æ—Ä
                    ]
                }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –º—É—Å–æ—Ä–∞
        elif '–ø—Ä–æ–±–µ–ª' in rule_lower and ('—É–¥–∞–ª–∏—Ç—å' in rule_lower or '–æ—á–∏—Å—Ç–∏—Ç—å' in rule_lower):
            return {
                'type': 'title_cleanup',
                'action': 'clean_spaces_and_garbage',
                'description': '–£–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –º—É—Å–æ—Ä –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏–π',
                'patterns_to_remove': [
                    r'\s+',  # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
                    r'^\s+|\s+$',  # –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
                    r'[^\w\s\-\.]'  # —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                ]
            }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω —É–º–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL
        elif 'url' in rule_lower and ('—Ñ–∞–π–ª' in rule_lower or '—Å–æ—Ü—Å–µ—Ç—å' in rule_lower or 'smm' in rule_lower):
            return {
                'type': 'url_filtering',
                'action': 'smart_url_filter',
                'description': '–û—Å—Ç–∞–≤–∏—Ç—å URL —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∞–π–ª–æ–≤, —Å–æ—Ü—Å–µ—Ç–µ–π –∏ SMM',
                'allowed_domains': ['yadi.sk', 'telegram.org', 'instagram.com', 'youtube.com', 'tiktok.com'],
                'condition': lambda url: any(domain in url.lower() for domain in ['yadi.sk', 'telegram.org', 'instagram.com', 'youtube.com', 'tiktok.com'])
            }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        elif 'wildberries' in rule_lower and '—É–¥–∞–ª–∏—Ç—å' in rule_lower:
            return {
                'type': 'link_cleanup',
                'action': 'remove_irrelevant_links',
                'description': '–£–¥–∞–ª–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ Wildberries –µ—Å–ª–∏ –Ω–µ –ø–æ —Ç–µ–º–µ',
                'irrelevant_domains': ['wildberries.ru', 'wildberries.com'],
                'condition': lambda url: any(domain in url.lower() for domain in ['wildberries.ru', 'wildberries.com'])
            }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ–≥–æ–≤ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        elif '—Ç–µ–≥' in rule_lower and ('smm' in rule_lower or '—Å–æ—Ü—Å–µ—Ç—å' in rule_lower):
            return {
                'type': 'tag_addition',
                'action': 'add_context_tags',
                'description': '–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–≥ SMM –µ—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å–æ—Ü—Å–µ—Ç–µ–π',
                'tag': 'SMM',
                'keywords': ['telegram', 'instagram', 'youtube', 'tiktok', '—Å–æ—Ü—Å–µ—Ç—å', 'smm']
            }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ–≥–æ–≤ –¥–ª—è –¥–∏–∑–∞–π–Ω–∞ –∏ –≥—Ä–∞—Ñ–∏–∫–∏
        elif '–¥–∏–∑–∞–π–Ω' in rule_lower and ('—Ç–µ–≥' in rule_lower or '–≤–∏–∑—É–∞–ª—å–Ω' in rule_lower):
            return {
                'type': 'tag_addition',
                'action': 'add_design_tags',
                'description': '–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–≥ –î–∏–∑–∞–π–Ω –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ö—É–∫–æ–≤ –∏ –≥—Ä–∞—Ñ–∏–∫–∏',
                'tag': '–î–∏–∑–∞–π–Ω',
                'keywords': ['–≤–∏–∑—É–∞–ª—å–Ω', '–≥—Ä–∞—Ñ–∏–∫', '—Ö—É–∫', '–¥–∏–∑–∞–π–Ω', '–ª–∏–ø—Å–∏–Ω–∫', 'hagiface']
            }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ–≥–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤
        elif ('–≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä' in rule_lower or '–ª–∏–ø—Å–∏–Ω–∫' in rule_lower or 'hagiface' in rule_lower):
            return {
                'type': 'tag_addition',
                'action': 'add_video_generator_tags',
                'description': '–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–≥–∏ –¥–ª—è –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π',
                'tags': ['–í–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã', '–ù–µ–π—Ä–æ—Å–µ—Ç–∏'],
                'keywords': ['–ª–∏–ø—Å–∏–Ω–∫', 'hagiface', '–≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä', '–Ω–µ–π—Ä–æ—Å–µ—Ç—å', 'ai', '–≥–µ–Ω–µ—Ä–∞—Ü–∏—è']
            }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
        elif '–≤–∞–∂–Ω–æ—Å—Ç—å' in rule_lower and re.search(r'\d+', rule):
            importance_match = re.search(r'(\d+)', rule)
            if importance_match:
                importance = int(importance_match.group(1))
                return {
                    'type': 'importance_setting',
                    'action': 'set_importance',
                    'description': f'–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å {importance}',
                    'importance': importance,
                    'context': rule
                }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –æ—á–∏—Å—Ç–∫–∏ —Ü–≤–µ—Ç–æ–≤ –∏ —Ü–≤–µ—Ç–æ–≤
        elif ('—Ü–≤–µ—Ç' in rule_lower or '—Ü–≤–µ—Ç–∞' in rule_lower) and '—É–¥–∞–ª–∏—Ç—å' in rule_lower:
            return {
                'type': 'content_cleanup',
                'action': 'remove_color_mentions',
                'description': '–£–¥–∞–ª–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ü–≤–µ—Ç–æ–≤ –µ—Å–ª–∏ –Ω–µ –ø–æ —Ç–µ–º–µ',
                'keywords': ['–∑–µ–ª–µ–Ω—ã–π', '–≥–æ–ª—É–±–æ–π', '–∫—Ä–∞—Å–Ω—ã–π', '—Å–∏–Ω–∏–π', '—Ü–≤–µ—Ç', '—Ü–≤–µ—Ç–∞'],
                'condition': lambda text: any(color in text.lower() for color in ['–∑–µ–ª–µ–Ω—ã–π', '–≥–æ–ª—É–±–æ–π', '–∫—Ä–∞—Å–Ω—ã–π', '—Å–∏–Ω–∏–π', '—Ü–≤–µ—Ç', '—Ü–≤–µ—Ç–∞'])
            }
        
        return None

    def _update_wisdom_base(self, new_rules: List[str]):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –±–∞–∑—É –º—É–¥—Ä–æ—Å—Ç–∏ –Ω–æ–≤—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏"""
        print("\nüß† –û–ë–ù–û–í–õ–ï–ù–ò–ï –ë–ê–ó–´ –ú–£–î–†–û–°–¢–ò")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for rule in new_rules:
            pattern_hash = hashlib.md5(rule.encode()).hexdigest()
            if pattern_hash not in [p.get('hash') for p in self.wisdom_base['patterns']]:
                self.wisdom_base['patterns'].append({
                    'hash': pattern_hash,
                    'rule': rule,
                    'timestamp': datetime.now().isoformat(),
                    'usage_count': 0,
                    'success_rate': 1.0
                })
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
        if 'user_preferences' not in self.wisdom_base:
            self.wisdom_base['user_preferences'] = {}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ —Ç–∏–ø–∞–º –ø—Ä–∞–≤–∏–ª
        rule_types = {}
        for rule in new_rules:
            rule_type = self._classify_rule_type(rule)
            if rule_type not in rule_types:
                rule_types[rule_type] = 0
            rule_types[rule_type] += 1
        
        for rule_type, count in rule_types.items():
            if rule_type not in self.wisdom_base['user_preferences']:
                self.wisdom_base['user_preferences'][rule_type] = 0
            self.wisdom_base['user_preferences'][rule_type] += count
        
        self._save_wisdom_base()
        print(f"‚úÖ –ë–∞–∑–∞ –º—É–¥—Ä–æ—Å—Ç–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(new_rules)} –Ω–æ–≤—ã—Ö –ø—Ä–∞–≤–∏–ª")

    def _classify_rule_type(self, rule: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –ø—Ä–∞–≤–∏–ª–∞"""
        rule_lower = rule.lower()
        
        if '—É–¥–∞–ª–∏—Ç—å' in rule_lower:
            return 'deletion'
        elif '—Ç–µ–≥' in rule_lower:
            return 'tagging'
        elif '–≤–∞–∂–Ω–æ—Å—Ç—å' in rule_lower:
            return 'importance'
        elif 'url' in rule_lower:
            return 'url_processing'
        elif '–æ—á–∏—Å—Ç–∏—Ç—å' in rule_lower or '—É–±—Ä–∞—Ç—å' in rule_lower:
            return 'cleanup'
        else:
            return 'general'

    def apply_enhanced_patterns(self):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–æ –≤—Å–µ–º –∑–∞–ø–∏—Å—è–º"""
        if not self.learned_patterns:
            try:
                self._load_training_data()
            except FileNotFoundError:
                print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ.")
                return
        
        print(f"üöÄ –ü–†–ò–ú–ï–ù–ï–ù–ò–ï {len(self.learned_patterns)} –£–õ–£–ß–®–ï–ù–ù–´–• –ü–ê–¢–¢–ï–†–ù–û–í")
        print("="*70)
        
        # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–Ω –∏–∑–º–µ–Ω–µ–Ω–∏–π
        changes_plan = []
        
        for page_id, analysis in self.analysis_data.items():
            record_changes = self._apply_enhanced_patterns_to_record(page_id, analysis)
            if record_changes:
                changes_plan.append(record_changes)
        
        print(f"\nüìä –ü–õ–ê–ù –ò–ó–ú–ï–ù–ï–ù–ò–ô: {len(changes_plan)} –∑–∞–ø–∏—Å–µ–π –±—É–¥—É—Ç –∏–∑–º–µ–Ω–µ–Ω—ã")
        
        if changes_plan:
            self._save_enhanced_changes_plan(changes_plan)
            print("‚úÖ –ü–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ enhanced_changes_plan.json")
            print("\nüéØ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
            print("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ enhanced_changes_plan.json")
            print("2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python enhanced_voice_training_system.py execute")
        else:
            print("‚ÑπÔ∏è –ò–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")

    def _apply_enhanced_patterns_to_record(self, page_id: str, analysis: Dict) -> Optional[Dict]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏"""
        changes = {
            'page_id': page_id,
            'current_title': analysis['current_title'],
            'current_description': analysis.get('current_description', ''),
            'current_tags': analysis.get('current_tags', []),
            'changes': {},
            'wisdom_applied': []
        }
        
        has_changes = False
        
        for pattern in self.learned_patterns:
            if pattern['type'] == 'record_deletion':
                if page_id == pattern.get('page_id'):
                    changes['changes']['delete'] = True
                    changes['wisdom_applied'].append(f"–£–¥–∞–ª–µ–Ω–∏–µ –ø–æ ID: {pattern['reason']}")
                    has_changes = True
            
            elif pattern['type'] == 'title_cleanup':
                new_title = self._apply_title_cleanup(pattern, analysis)
                if new_title and new_title != analysis['current_title']:
                    changes['changes']['title'] = new_title
                    changes['wisdom_applied'].append(f"–û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è: {pattern['description']}")
                    has_changes = True
            
            elif pattern['type'] == 'url_filtering':
                new_urls = self._apply_url_filtering(pattern, analysis)
                if new_urls:
                    changes['changes']['filtered_urls'] = new_urls
                    changes['wisdom_applied'].append("–£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è URL")
                    has_changes = True
            
            elif pattern['type'] == 'link_cleanup':
                cleaned_content = self._apply_link_cleanup(pattern, analysis)
                if cleaned_content:
                    changes['changes']['cleaned_content'] = cleaned_content
                    changes['wisdom_applied'].append("–û—á–∏—Å—Ç–∫–∞ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
                    has_changes = True
            
            elif pattern['type'] == 'tag_addition':
                new_tags = self._apply_enhanced_tagging(pattern, analysis)
                if new_tags:
                    changes['changes']['tags'] = new_tags
                    changes['wisdom_applied'].append(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤: {pattern.get('tag', pattern.get('tags', []))}")
                    has_changes = True
            
            elif pattern['type'] == 'importance_setting':
                changes['changes']['importance'] = pattern['importance']
                changes['wisdom_applied'].append(f"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ {pattern['importance']}")
                has_changes = True
        
        return changes if has_changes else None

    def _apply_title_cleanup(self, pattern: Dict, analysis: Dict) -> Optional[str]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ—á–∏—Å—Ç–∫—É –Ω–∞–∑–≤–∞–Ω–∏–π"""
        title = analysis['current_title']
        
        if pattern['action'] == 'clean_gibberish':
            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –±–∏–ª–∏–±–µ—Ä–¥—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–ø–∏—Å–∏
            if pattern.get('page_id') and analysis.get('page_id') == pattern['page_id']:
                for pattern_to_remove in pattern['patterns_to_remove']:
                    title = re.sub(pattern_to_remove, ' ', title, flags=re.IGNORECASE)
                return ' '.join(title.split())  # —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        
        elif pattern['action'] == 'clean_spaces_and_garbage':
            # –û–±—â–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –º—É—Å–æ—Ä–∞
            for pattern_to_remove in pattern['patterns_to_remove']:
                title = re.sub(pattern_to_remove, ' ', title)
            return ' '.join(title.split())  # —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        
        return None

    def _apply_enhanced_tagging(self, pattern: Dict, analysis: Dict) -> Optional[List[str]]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        content = f"{analysis['current_title']} {analysis.get('current_description', '')}".lower()
        current_tags = analysis.get('current_tags', [])
        new_tags = current_tags.copy()
        
        if pattern['action'] == 'add_context_tags':
            # –û–±—ã—á–Ω–æ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            for keyword in pattern['keywords']:
                if keyword.lower() in content:
                    if pattern['tag'] not in new_tags:
                        new_tags.append(pattern['tag'])
                    break
        
        elif pattern['action'] == 'add_design_tags':
            # –¢–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏–∑–∞–π–Ω–∞
            for keyword in pattern['keywords']:
                if keyword.lower() in content:
                    if pattern['tag'] not in new_tags:
                        new_tags.append(pattern['tag'])
                    break
        
        elif pattern['action'] == 'add_video_generator_tags':
            # –¢–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤
            for keyword in pattern['keywords']:
                if keyword.lower() in content:
                    for tag in pattern['tags']:
                        if tag not in new_tags:
                            new_tags.append(tag)
                    break
        
        return new_tags if new_tags != current_tags else None

    def _apply_url_filtering(self, pattern: Dict, analysis: Dict) -> Optional[List[str]]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —É–º–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é URL"""
        links = analysis.get('extracted_links', [])
        if not links:
            return None
        
        filtered_links = []
        for link in links:
            if pattern['condition'](link):
                filtered_links.append(link)
        
        return filtered_links if len(filtered_links) != len(links) else None

    def _apply_link_cleanup(self, pattern: Dict, analysis: Dict) -> Optional[Dict]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ—á–∏—Å—Ç–∫—É –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        description = analysis.get('current_description', '')
        if not description:
            return None
        
        cleaned_description = description
        for domain in pattern['irrelevant_domains']:
            # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
            cleaned_description = re.sub(rf'https?://[^\s]*{domain}[^\s]*', '', cleaned_description)
        
        return {'description': cleaned_description.strip()} if cleaned_description != description else None

    def _save_enhanced_changes_plan(self, changes_plan: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–ª–∞–Ω –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        plan_data = {
            'timestamp': datetime.now().isoformat(),
            'total_changes': len(changes_plan),
            'wisdom_applied': True,
            'llm_integration': {name: info['available'] for name, info in self.llm_processors.items()},
            'changes': changes_plan
        }
        
        with open('enhanced_changes_plan.json', 'w', encoding='utf-8') as f:
            json.dump(plan_data, f, ensure_ascii=False, indent=2)

    def _save_training_data(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è"""
        training_data = {
            'timestamp': datetime.now().isoformat(),
            'patterns': [],
            'wisdom_integration': True
        }
        
        for pattern in self.learned_patterns:
            serializable_pattern = {
                'type': pattern['type'],
                'action': pattern['action'],
                'description': pattern['description']
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è
            for key in ['page_id', 'reason', 'allowed_domains', 'irrelevant_domains', 'tag', 'keywords', 'importance']:
                if key in pattern:
                    serializable_pattern[key] = pattern[key]
            
            training_data['patterns'].append(serializable_pattern)
        
        with open('enhanced_voice_training_data.json', 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)

    def _load_training_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è"""
        with open('enhanced_voice_training_data.json', 'r', encoding='utf-8') as f:
            training_data = json.load(f)
        
        self.learned_patterns = []
        for pattern_data in training_data['patterns']:
            pattern = {
                'type': pattern_data['type'],
                'action': pattern_data['action'],
                'description': pattern_data['description']
            }
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è
            for key in ['page_id', 'reason', 'allowed_domains', 'irrelevant_domains', 'tag', 'keywords', 'importance']:
                if key in pattern_data:
                    pattern[key] = pattern_data[key]
            
            self.learned_patterns.append(pattern)

    def _show_learned_patterns(self):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–∑—É—á–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        print("\nüß† –ò–ó–£–ß–ï–ù–ù–´–ï –£–õ–£–ß–®–ï–ù–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´:")
        print("-" * 50)
        for i, pattern in enumerate(self.learned_patterns, 1):
            print(f"{i}. {pattern['description']}")
            if 'wisdom_applied' in pattern:
                print(f"   üß† –ú—É–¥—Ä–æ—Å—Ç—å: {pattern['wisdom_applied']}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  python enhanced_voice_training_system.py learn    # –û–±—É—á–µ–Ω–∏–µ")
        print("  python enhanced_voice_training_system.py apply    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ")
        print("  python enhanced_voice_training_system.py execute  # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ")
        return
    
    command = sys.argv[1]
    system = EnhancedVoiceTrainingSystem()
    
    if command == "learn":
        system.start_enhanced_voice_learning()
    elif command == "apply":
        system.apply_enhanced_patterns()
    elif command == "execute":
        print("üöÄ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π...")
        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –ª–æ–≥–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
    else:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {command}")

if __name__ == "__main__":
    main() 