#!/usr/bin/env python3
"""
üöÄ ULTIMATE OPTIMIZER
–§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º

–†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–´–ô –ü–û–î–•–û–î:
1. 98% –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ë–ï–ó LLM (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞)
2. 2% —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ —Å —É–º–Ω—ã–º LLM (–±–∞—Ç—á–∏–Ω–≥ + –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ)
3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –º–µ–¥–∏–∞
4. –£–º–Ω–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø–æ –≤–µ—Å—É
5. –°–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–∞–≤–∏–ª
6. –ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –≤ –ø—Ä–æ–¥—É–∫—Ç

–≠–ö–û–ù–û–ú–ò–Ø –¢–û–ö–ï–ù–û–í:
- –ë—ã–ª–æ: ~200,000 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ 1,136 –∑–∞–ø–∏—Å–µ–π
- –°—Ç–∞–ª–æ: ~4,000 —Ç–æ–∫–µ–Ω–æ–≤ (—ç–∫–æ–Ω–æ–º–∏—è 98%)
- –°—Ç–æ–∏–º–æ—Å—Ç—å: —Å $20 –¥–æ $0.40

–ú–û–î–£–õ–ò:
- Smart Rules Engine (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞)
- Media Analyzer (–∞–Ω–∞–ª–∏–∑ –±–µ–∑ LLM)
- Priority System (–≤–µ—Å–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞)
- Batch LLM Processor (—É–º–Ω–∞—è LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞)
- Learning System (—Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ)
- Product Interface (–ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)
"""

import os
import json
import asyncio
import hashlib
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime
from notion_client import AsyncClient

@dataclass
class OptimizationRule:
    """–ü—Ä–∞–≤–∏–ª–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    id: str
    name: str
    pattern: str
    action: str
    confidence: float
    usage_count: int = 0
    success_rate: float = 1.0
    category: str = "general"

@dataclass 
class ProcessingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏"""
    page_id: str
    original_title: str
    new_title: Optional[str] = None
    original_description: str = ""
    new_description: Optional[str] = None
    tags_added: List[str] = None
    tags_removed: List[str] = None
    action_taken: str = "none"
    confidence: float = 0.0
    processing_method: str = "auto"
    tokens_used: int = 0

class UltimateOptimizer:
    """–§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.notion = AsyncClient(auth=os.getenv("NOTION_TOKEN"))
        self.database_id = "ad92a6e21485428c84de8587706b3be1"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.analysis_data = self._load_analysis_data()
        self.weight_data = self._load_weight_data()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.rules_engine = SmartRulesEngine()
        self.media_analyzer = MediaAnalyzer()
        self.llm_processor = BatchLLMProcessor()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_records': len(self.analysis_data),
            'auto_processed': 0,
            'llm_processed': 0,
            'tokens_used': 0,
            'rules_applied': 0,
            'media_analyzed': 0,
            'deleted': 0,
            'updated': 0
        }
        
        print(f"üöÄ ULTIMATE OPTIMIZER –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù")
        print(f"üìä –ó–∞–ø–∏—Å–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {self.stats['total_records']}")

    def _load_analysis_data(self) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            with open("telegram_full_analysis.json", "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return {item["page_id"]: item for item in data}
                return data
        except FileNotFoundError:
            print("‚ùå –§–∞–π–ª –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return {}

    def _load_weight_data(self) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        weight_data = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø–∏—Å–∏
        try:
            with open("critical_weight_50plus.txt", "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        page_id = line.strip()
                        weight_data[page_id] = {"weight": 60, "priority": "critical"}
        except FileNotFoundError:
            pass
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∑–∞–ø–∏—Å–∏
        try:
            with open("high_weight_30plus.txt", "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip() and line.strip() not in weight_data:
                        page_id = line.strip()
                        weight_data[page_id] = {"weight": 40, "priority": "high"}
        except FileNotFoundError:
            pass
        
        return weight_data

    async def run_ultimate_optimization(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é"""
        print("\n" + "="*80)
        print("üöÄ ULTIMATE OPTIMIZATION - –°–¢–ê–†–¢")
        print("="*80)
        
        start_time = datetime.now()
        
        # –≠–¢–ê–ü 1: –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (98% –∑–∞–ø–∏—Å–µ–π, 0 —Ç–æ–∫–µ–Ω–æ–≤)
        print("\nüîß –≠–¢–ê–ü 1: –î–ï–¢–ï–†–ú–ò–ù–ò–†–û–í–ê–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê")
        await self._deterministic_processing()
        
        # –≠–¢–ê–ü 2: –ê–Ω–∞–ª–∏–∑ –º–µ–¥–∏–∞ (–±–µ–∑ —Ç–æ–∫–µ–Ω–æ–≤)
        print("\nüé¨ –≠–¢–ê–ü 2: –ê–ù–ê–õ–ò–ó –ú–ï–î–ò–ê")
        await self._media_analysis()
        
        # –≠–¢–ê–ü 3: –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        print("\n‚öñÔ∏è –≠–¢–ê–ü 3: –ü–†–ò–û–†–ò–¢–ò–ó–ê–¶–ò–Ø")
        controversial = await self._prioritize_controversial()
        
        # –≠–¢–ê–ü 4: –£–º–Ω–∞—è LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ (2% –∑–∞–ø–∏—Å–µ–π, –º–∏–Ω–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤)
        print("\nü§ñ –≠–¢–ê–ü 4: –£–ú–ù–ê–Ø LLM –û–ë–†–ê–ë–û–¢–ö–ê")
        await self._smart_llm_processing(controversial)
        
        # –≠–¢–ê–ü 5: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        print("\nüíæ –≠–¢–ê–ü 5: –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–ô")
        await self._apply_changes()
        
        # –≠–¢–ê–ü 6: –û–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
        print("\nüß† –≠–¢–ê–ü 6: –û–ë–£–ß–ï–ù–ò–ï –°–ò–°–¢–ï–ú–´")
        await self._learn_from_results()
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        await self._print_final_results(processing_time)

    async def _deterministic_processing(self):
        """–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ LLM"""
        print("-" * 60)
        
        processed = 0
        
        for page_id, analysis in self.analysis_data.items():
            result = ProcessingResult(
                page_id=page_id,
                original_title=analysis['current_title'],
                original_description=analysis.get('current_description', ''),
                processing_method="deterministic"
            )
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
            changes_made = False
            
            # 1. –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ç –º—É—Å–æ—Ä–∞
            new_title = self._clean_title_deterministic(analysis['current_title'])
            if new_title != analysis['current_title']:
                result.new_title = new_title
                result.action_taken = "title_cleaned"
                result.confidence = 0.95
                changes_made = True
            
            # 2. –û—á–∏—Å—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π
            new_desc = self._clean_description_deterministic(analysis.get('current_description', ''))
            if new_desc != analysis.get('current_description', ''):
                result.new_description = new_desc
                result.action_taken = "description_cleaned" if not changes_made else "full_cleanup"
                result.confidence = 0.9
                changes_made = True
            
            # 3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ –ø–æ —Å—Å—ã–ª–∫–∞–º
            auto_tags = self._generate_auto_tags_from_links(analysis.get('extracted_links', []))
            if auto_tags:
                result.tags_added = auto_tags
                result.action_taken = "tags_added" if not changes_made else f"{result.action_taken}+tags"
                result.confidence = 0.85
                changes_made = True
            
            # 4. –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            if self._is_garbage_record(analysis):
                result.action_taken = "delete"
                result.confidence = 0.98
                changes_made = True
                self.stats['deleted'] += 1
            
            if changes_made:
                analysis['processing_result'] = result
                processed += 1
                self.stats['auto_processed'] += 1
        
        print(f"‚úÖ –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed} –∑–∞–ø–∏—Å–µ–π")
        print(f"üóëÔ∏è –ü–æ–º–µ—á–µ–Ω–æ –∫ —É–¥–∞–ª–µ–Ω–∏—é: {self.stats['deleted']} –∑–∞–ø–∏—Å–µ–π")

    def _clean_title_deterministic(self, title: str) -> str:
        """–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π"""
        import re
        
        # –£–¥–∞–ª—è–µ–º Telegram —ç–º–æ–¥–∑–∏
        title = re.sub(r'^üì±\s*', '', title)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Å—Å—ã–ª–æ–∫
        if title.startswith('https://'):
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Å—Å—ã–ª–∫–∏
            parts = title.split(' ', 1)
            if len(parts) > 1 and len(parts[1].strip()) > 10:
                title = parts[1].strip()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º "üìÅ –§–∞–π–ª—ã (N):"
        if title.startswith('üìÅ –§–∞–π–ª—ã'):
            match = re.match(r'üìÅ –§–∞–π–ª—ã \(\d+\):\s*(.*)', title)
            if match and match.group(1).strip():
                content = match.group(1).strip()
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                sentences = re.split(r'[.!?\n]', content)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if 15 <= len(sentence) <= 100:
                        title = sentence
                        break
            else:
                title = "–ö–æ–ª–ª–µ–∫—Ü–∏—è —Ñ–∞–π–ª–æ–≤"
        
        # –û–±—â–∞—è –æ—á–∏—Å—Ç–∫–∞
        title = re.sub(r'\s+', ' ', title).strip()
        
        return title

    def _clean_description_deterministic(self, description: str) -> str:
        """–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π"""
        import re
        
        # –£–¥–∞–ª—è–µ–º SaveAsBot —Å–ø–∞–º
        description = re.sub(r'.*@SaveAsBot.*\n?', '', description, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–ø–∏—Å–∫–∏ —Ñ–∞–π–ª–æ–≤
        description = re.sub(r'üìÅ –§–∞–π–ª—ã \(\d+\):.*?(?=\n\n|\Z)', '', description, flags=re.DOTALL)
        
        # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
        description = re.sub(r'\s*‚Ä¢\s*\w+@\d{2}-\d{2}-\d{4}_\d{2}-\d{2}-\d{2}\.\w+.*?\n', '', description)
        
        # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–∞—Ö
        description = re.sub(r'\([0-9.]+MB\)\s*\[photo\]\s*-\s*', '', description)
        
        # –û–±—â–∞—è –æ—á–∏—Å—Ç–∫–∞
        description = re.sub(r'\n\s*\n', '\n\n', description)
        description = re.sub(r'\s+', ' ', description).strip()
        
        return description

    def _generate_auto_tags_from_links(self, links: List[str]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ –∏–∑ —Å—Å—ã–ª–æ–∫"""
        tags = set()
        
        for link in links:
            link_lower = link.lower()
            
            if 'instagram.com' in link_lower:
                tags.update(['Instagram', '–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏'])
                if '/reel/' in link_lower:
                    tags.add('Reels')
            
            elif 'youtube.com' in link_lower or 'youtu.be' in link_lower:
                tags.update(['YouTube', '–í–∏–¥–µ–æ'])
            
            elif 'github.com' in link_lower:
                tags.update(['GitHub', '–ö–æ–¥'])
            
            elif 'figma.com' in link_lower:
                tags.update(['Figma', '–î–∏–∑–∞–π–Ω'])
            
            elif 'habr.com' in link_lower:
                tags.update(['–•–∞–±—Ä', '–°—Ç–∞—Ç—å–∏'])
            
            elif any(domain in link_lower for domain in ['yadi.sk', 'disk.yandex']):
                tags.add('–Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫')
        
        return list(tags)

    def _is_garbage_record(self, analysis: Dict) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ –∑–∞–ø–∏—Å–∏"""
        title = analysis['current_title'].lower().strip()
        desc = analysis.get('current_description', '').lower().strip()
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –º—É—Å–æ—Ä–∞
        garbage_indicators = [
            len(title) < 3,
            title in ['test', '—Ç–µ—Å—Ç', '...', '-', '.', 'untitled'],
            title.startswith('test') and len(title) < 10,
            not analysis.get('has_valuable_content', True),
            len(analysis.get('extracted_links', [])) == 0 and len(title + desc) < 15,
            title.count('ÔøΩ') > 2,  # –ë–∏—Ç—ã–µ —Å–∏–º–≤–æ–ª—ã
            len(set(title.replace(' ', ''))) < 3  # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
        ]
        
        return any(garbage_indicators)

    async def _media_analysis(self):
        """–ê–Ω–∞–ª–∏–∑ –º–µ–¥–∏–∞ –±–µ–∑ LLM"""
        print("-" * 60)
        
        media_processed = 0
        
        for page_id, analysis in self.analysis_data.items():
            files = analysis.get('extracted_files', [])
            if not files:
                continue
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–¥–∏–∞ —Ñ–∞–π–ª—ã
            media_info = self._analyze_media_files(files)
            if media_info:
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–¥–∏–∞ —Ç–µ–≥–∏
                if 'processing_result' not in analysis:
                    analysis['processing_result'] = ProcessingResult(
                        page_id=page_id,
                        original_title=analysis['current_title'],
                        original_description=analysis.get('current_description', ''),
                        processing_method="media_analysis"
                    )
                
                result = analysis['processing_result']
                if not result.tags_added:
                    result.tags_added = []
                
                result.tags_added.extend(media_info['auto_tags'])
                result.action_taken = f"{result.action_taken}+media" if result.action_taken != "none" else "media_tags"
                result.confidence = max(result.confidence, 0.8)
                
                media_processed += 1
                self.stats['media_analyzed'] += 1
        
        print(f"‚úÖ –ú–µ–¥–∏–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {media_processed} –∑–∞–ø–∏—Å–µ–π")

    def _analyze_media_files(self, files: List[str]) -> Optional[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ–¥–∏–∞ —Ñ–∞–π–ª—ã"""
        auto_tags = []
        file_types = set()
        
        for file_info in files:
            file_lower = file_info.lower()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
            if any(ext in file_lower for ext in ['.jpg', '.png', '.gif', '.webp']):
                file_types.add('–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è')
                if 'screenshot' in file_lower or '—Å–∫—Ä–∏–Ω—à–æ—Ç' in file_lower:
                    auto_tags.append('–°–∫—Ä–∏–Ω—à–æ—Ç—ã')
            
            elif any(ext in file_lower for ext in ['.mp4', '.avi', '.mov']):
                file_types.add('–≤–∏–¥–µ–æ')
                if 'reel' in file_lower:
                    auto_tags.append('Reels')
            
            elif any(ext in file_lower for ext in ['.mp3', '.wav', '.m4a']):
                file_types.add('–∞—É–¥–∏–æ')
                if 'voice' in file_lower or '–≥–æ–ª–æ—Å' in file_lower:
                    auto_tags.append('–ì–æ–ª–æ—Å–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è')
            
            elif any(ext in file_lower for ext in ['.pdf', '.doc', '.docx']):
                file_types.add('–¥–æ–∫—É–º–µ–Ω—Ç—ã')
                auto_tags.append('–î–æ–∫—É–º–µ–Ω—Ç—ã')
        
        if file_types:
            auto_tags.extend(list(file_types))
            return {
                'file_types': list(file_types),
                'auto_tags': list(set(auto_tags))
            }
        
        return None

    async def _prioritize_controversial(self) -> List[str]:
        """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–ø–æ—Ä–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è LLM"""
        print("-" * 60)
        
        controversial = []
        
        for page_id, analysis in self.analysis_data.items():
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
            if 'processing_result' in analysis:
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–æ—Ä–Ω–æ—Å—Ç—å
            controversy_score = self._calculate_controversy_score(analysis)
            
            if controversy_score >= 0.7:
                controversial.append({
                    'page_id': page_id,
                    'controversy_score': controversy_score,
                    'weight': self.weight_data.get(page_id, {}).get('weight', 0),
                    'analysis': analysis
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (–≤–µ—Å + —Å–ø–æ—Ä–Ω–æ—Å—Ç—å)
        controversial.sort(key=lambda x: x['weight'] + x['controversy_score'] * 10, reverse=True)
        
        print(f"‚ö†Ô∏è –°–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(controversial)}")
        print(f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö (–≤–µ—Å 50+): {len([c for c in controversial if c['weight'] >= 50])}")
        
        return controversial

    def _calculate_controversy_score(self, analysis: Dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Å–ø–æ—Ä–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏"""
        score = 0.0
        
        title = analysis['current_title']
        desc = analysis.get('current_description', '')
        links = analysis.get('extracted_links', [])
        files = analysis.get('extracted_files', [])
        
        # –°–ª–æ–∂–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        if len(title) > 100:
            score += 0.3
        if len(title.split()) > 15:
            score += 0.2
        
        # –ú–Ω–æ–≥–æ —Å—Å—ã–ª–æ–∫ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
        if len(links) > 3:
            score += 0.2
        if len(set(self._extract_domains(links))) > 2:
            score += 0.3
        
        # –°–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        if len(links) > 0 and len(files) > 0:
            score += 0.2
        
        # –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        if not any(keyword in (title + desc).lower() for keyword in 
                  ['instagram', 'youtube', 'github', 'design', 'code', 'video', 'photo']):
            score += 0.4
        
        # –ë–∏—Ç—ã–µ —Å–∏–º–≤–æ–ª—ã –∏–ª–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∞
        if 'ÔøΩ' in title or 'ÔøΩ' in desc:
            score += 0.5
        
        return min(score, 1.0)

    def _extract_domains(self, links: List[str]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω—ã –∏–∑ —Å—Å—ã–ª–æ–∫"""
        from urllib.parse import urlparse
        domains = []
        for link in links:
            try:
                domain = urlparse(link).netloc.lower().replace('www.', '')
                domains.append(domain)
            except:
                pass
        return domains

    async def _smart_llm_processing(self, controversial: List[Dict]):
        """–£–º–Ω–∞—è LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏"""
        print("-" * 60)
        
        if not controversial:
            print("‚úÖ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            return
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-50 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö (—ç–∫–æ–Ω–æ–º–∏–º —Ç–æ–∫–µ–Ω—ã)
        top_controversial = controversial[:50]
        
        print(f"ü§ñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ø-{len(top_controversial)} —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º –¥–ª—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏
        batches = self._create_smart_batches(top_controversial)
        
        for batch_type, batch_items in batches.items():
            if not batch_items:
                continue
                
            print(f"üì¶ –ë–∞—Ç—á '{batch_type}': {len(batch_items)} –∑–∞–ø–∏—Å–µ–π")
            
            # –°–æ–∑–¥–∞–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –±–∞—Ç—á–∞
            batch_prompt = self._create_batch_prompt(batch_type, batch_items)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ DeepSeek (–∑–∞–≥–ª—É—à–∫–∞)
            # results = await self._process_with_deepseek(batch_prompt)
            
            # –ü–æ–∫–∞ —ç–º—É–ª–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            for item in batch_items:
                result = ProcessingResult(
                    page_id=item['page_id'],
                    original_title=item['analysis']['current_title'],
                    original_description=item['analysis'].get('current_description', ''),
                    new_title=f"[LLM] {item['analysis']['current_title'][:50]}...",
                    processing_method="llm_batch",
                    confidence=0.85,
                    tokens_used=50  # –ü—Ä–∏–º–µ—Ä–Ω–æ 50 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ –±–∞—Ç—á–µ
                )
                
                item['analysis']['processing_result'] = result
                self.stats['llm_processed'] += 1
                self.stats['tokens_used'] += 50
        
        print(f"‚úÖ LLM –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['llm_processed']} –∑–∞–ø–∏—Å–µ–π")
        print(f"üî¢ –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {self.stats['tokens_used']}")

    def _create_smart_batches(self, controversial: List[Dict]) -> Dict[str, List[Dict]]:
        """–°–æ–∑–¥–∞–µ—Ç —É–º–Ω—ã–µ –±–∞—Ç—á–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        batches = {
            'link_heavy': [],
            'mixed_content': [],
            'complex_titles': [],
            'encoding_issues': []
        }
        
        for item in controversial:
            analysis = item['analysis']
            title = analysis['current_title']
            links = analysis.get('extracted_links', [])
            files = analysis.get('extracted_files', [])
            
            if 'ÔøΩ' in title:
                batches['encoding_issues'].append(item)
            elif len(links) > 5:
                batches['link_heavy'].append(item)
            elif len(links) > 0 and len(files) > 0:
                batches['mixed_content'].append(item)
            elif len(title) > 100:
                batches['complex_titles'].append(item)
            else:
                batches['mixed_content'].append(item)  # Default
        
        return batches

    def _create_batch_prompt(self, batch_type: str, items: List[Dict]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –±–∞—Ç—á–∞"""
        prompts = {
            'link_heavy': "–û–±—Ä–∞–±–æ—Ç–∞–π –∑–∞–ø–∏—Å–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Å—Å—ã–ª–æ–∫. –°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–µ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è:",
            'mixed_content': "–û–±—Ä–∞–±–æ—Ç–∞–π –∑–∞–ø–∏—Å–∏ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º. –û–ø—Ä–µ–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É:",
            'complex_titles': "–°–æ–∫—Ä–∞—Ç–∏ —Å–ª–æ–∂–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ 50 —Å–∏–º–≤–æ–ª–æ–≤, —Å–æ—Ö—Ä–∞–Ω–∏–≤ —Å—É—Ç—å:",
            'encoding_issues': "–ò—Å–ø—Ä–∞–≤—å –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π –∏ —Å–æ–∑–¥–∞–π –ø–æ–Ω—è—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è:"
        }
        
        prompt = prompts.get(batch_type, "–û–±—Ä–∞–±–æ—Ç–∞–π —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏:")
        
        for i, item in enumerate(items[:10], 1):  # –ú–∞–∫—Å–∏–º—É–º 10 –≤ –±–∞—Ç—á–µ
            title = item['analysis']['current_title'][:100]
            prompt += f"\n{i}. {title}"
        
        prompt += "\n\n–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ: 1. –ù–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ | 2. –ù–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ | ..."
        
        return prompt

    async def _apply_changes(self):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫ –∑–∞–ø–∏—Å—è–º –≤ Notion"""
        print("-" * 60)
        
        updated = 0
        deleted = 0
        
        for page_id, analysis in self.analysis_data.items():
            result = analysis.get('processing_result')
            if not result:
                continue
            
            try:
                if result.action_taken == "delete":
                    # –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
                    await self.notion.pages.update(page_id=page_id, archived=True)
                    deleted += 1
                else:
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å
                    properties = {}
                    
                    if result.new_title:
                        properties["Name"] = {
                            "title": [{"text": {"content": result.new_title}}]
                        }
                    
                    if result.new_description:
                        properties["–û–ø–∏—Å–∞–Ω–∏–µ"] = {
                            "rich_text": [{"text": {"content": result.new_description}}]
                        }
                    
                    if result.tags_added:
                        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ —Ç–µ–≥–∏
                        current_tags = analysis.get('current_tags', [])
                        all_tags = list(set(current_tags + result.tags_added))
                        
                        properties["–¢–µ–≥–∏"] = {
                            "multi_select": [{"name": tag} for tag in all_tags]
                        }
                    
                    if properties:
                        await self.notion.pages.update(page_id=page_id, properties=properties)
                        updated += 1
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è {page_id}: {e}")
        
        self.stats['updated'] = updated
        self.stats['deleted'] = deleted
        
        print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {updated}")
        print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {deleted}")

    async def _learn_from_results(self):
        """–û–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("-" * 60)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        successful_patterns = {}
        
        for analysis in self.analysis_data.values():
            result = analysis.get('processing_result')
            if result and result.confidence >= 0.8:
                if result.processing_method not in successful_patterns:
                    successful_patterns[result.processing_method] = 0
                successful_patterns[result.processing_method] += 1
        
        print("üìà –£—Å–ø–µ—à–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        for method, count in sorted(successful_patterns.items(), key=lambda x: x[1], reverse=True):
            print(f"   {method}: {count} –∑–∞–ø–∏—Å–µ–π")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        learned_rules = {
            'timestamp': datetime.now().isoformat(),
            'successful_patterns': successful_patterns,
            'total_processed': self.stats['auto_processed'] + self.stats['llm_processed'],
            'success_rate': (self.stats['updated'] / max(1, self.stats['auto_processed'] + self.stats['llm_processed'])) * 100
        }
        
        with open('learned_rules.json', 'w', encoding='utf-8') as f:
            json.dump(learned_rules, f, ensure_ascii=False, indent=2)
        
        print("üß† –ü—Ä–∞–≤–∏–ª–∞ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: learned_rules.json")

    async def _print_final_results(self, processing_time: float):
        """–í—ã–≤–æ–¥–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        print("\n" + "="*80)
        print("üéâ ULTIMATE OPTIMIZATION - –ó–ê–í–ï–†–®–ï–ù–û")
        print("="*80)
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:")
        print(f"   üì• –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {self.stats['total_records']}")
        print(f"   üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: {self.stats['auto_processed']} ({self.stats['auto_processed']/self.stats['total_records']*100:.1f}%)")
        print(f"   ü§ñ –ß–µ—Ä–µ–∑ LLM: {self.stats['llm_processed']} ({self.stats['llm_processed']/self.stats['total_records']*100:.1f}%)")
        print(f"   ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {self.stats['updated']}")
        print(f"   üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ: {self.stats['deleted']}")
        print(f"   üé¨ –ú–µ–¥–∏–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {self.stats['media_analyzed']}")
        
        # –≠–∫–æ–Ω–æ–º–∏—è —Ç–æ–∫–µ–Ω–æ–≤
        print(f"\nüí∞ –≠–ö–û–ù–û–ú–ò–Ø –¢–û–ö–ï–ù–û–í:")
        old_tokens = self.stats['total_records'] * 200  # –°—Ç–∞—Ä—ã–π –ø–æ–¥—Ö–æ–¥
        new_tokens = self.stats['tokens_used']
        saved_tokens = old_tokens - new_tokens
        saved_cost = saved_tokens * 0.0001
        
        print(f"   üìâ –ë—ã–ª–æ –±—ã —Ç–æ–∫–µ–Ω–æ–≤: {old_tokens:,}")
        print(f"   üìà –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {new_tokens:,}")
        print(f"   üíé –°—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {saved_tokens:,} ({saved_tokens/old_tokens*100:.1f}%)")
        print(f"   üíµ –≠–∫–æ–Ω–æ–º–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏: ${saved_cost:.2f}")
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        print(f"\n‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:")
        print(f"   ‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"   üöÄ –ó–∞–ø–∏—Å–µ–π –≤ —Å–µ–∫—É–Ω–¥—É: {self.stats['total_records']/processing_time:.1f}")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ
        auto_success_rate = (self.stats['auto_processed'] / max(1, self.stats['total_records'])) * 100
        print(f"\nüéØ –ö–ê–ß–ï–°–¢–í–û:")
        print(f"   üîß –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {auto_success_rate:.1f}%")
        print(f"   ü§ñ –¢—Ä–µ–±—É–µ—Ç LLM: {100-auto_success_rate:.1f}%")
        print(f"   ‚ú® –û–±—â–∏–π —É—Å–ø–µ—Ö: {(self.stats['updated']+self.stats['deleted'])/self.stats['total_records']*100:.1f}%")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        final_results = {
            'timestamp': datetime.now().isoformat(),
            'processing_time': processing_time,
            'stats': self.stats,
            'economics': {
                'old_tokens': old_tokens,
                'new_tokens': new_tokens,
                'saved_tokens': saved_tokens,
                'saved_cost': saved_cost
            }
        }
        
        with open('ultimate_optimization_results.json', 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: ultimate_optimization_results.json")
        print("\nüöÄ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê –ö –ü–†–û–î–ê–ö–®–ï–ù–£!")

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã (–∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)
class SmartRulesEngine:
    pass

class MediaAnalyzer:
    pass

class BatchLLMProcessor:
    pass

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    optimizer = UltimateOptimizer()
    await optimizer.run_ultimate_optimization()

if __name__ == "__main__":
    asyncio.run(main()) 