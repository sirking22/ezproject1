# üß† –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π Llama 70B

## üéØ –û–±–∑–æ—Ä

–≠—Ç–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ–º–æ–∂–µ—Ç –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é Llama 70B –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π AI-—ç–∫–æ—Å–∏—Å—Ç–µ–º—ã. –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:

- **–ü—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å** - –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ—Å—Ç–∞—é—Ç—Å—è –Ω–∞ —Ç–≤–æ–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ
- **–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é** - –º–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ–¥ —Ç–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ—Å—Ç—å** - –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —Ä–∞–±–æ—á–∏–º –∏ –¥–æ–º–∞—à–Ω–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏
- **–ù–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å** - —Ä–∞–±–æ—Ç–∞ –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞

---

## üñ•Ô∏è –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- **RAM**: 16 GB (–¥–ª—è 4-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è)
- **GPU**: NVIDIA GPU —Å 8GB VRAM (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- **CPU**: 8+ —è–¥–µ—Ä
- **–î–∏—Å–∫**: 50 GB —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- **RAM**: 32 GB
- **GPU**: NVIDIA RTX 4090 –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è
- **CPU**: 16+ —è–¥–µ—Ä
- **–î–∏—Å–∫**: 100 GB SSD

---

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ llama-cpp-python

```bash
# –ë–∞–∑–æ–≤–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install llama-cpp-python

# –° –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA (–µ—Å–ª–∏ –µ—Å—Ç—å NVIDIA GPU)
pip install llama-cpp-python --force-reinstall --index-url https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu118

# –° –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Metal (–¥–ª—è Mac —Å Apple Silicon)
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
```

### 2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏

```bash
# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
mkdir -p models
cd models

# –°–∫–∞—á–∏–≤–∞–µ–º Llama 70B –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é (4-bit)
wget https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/resolve/main/llama-2-70b-chat.Q4_K_M.gguf

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ, –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç—ã (7B –º–æ–¥–µ–ª—å)
wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf
```

### 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install fastapi uvicorn aiohttp
```

---

## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞

### 1. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞

–°–æ–∑–¥–∞–π —Ñ–∞–π–ª `llm_config.json`:

```json
{
  "models": {
    "fast": {
      "path": "models/llama-2-7b-chat.Q4_K_M.gguf",
      "n_ctx": 4096,
      "n_threads": 8,
      "n_gpu_layers": 0
    },
    "default": {
      "path": "models/llama-2-70b-chat.Q4_K_M.gguf", 
      "n_ctx": 4096,
      "n_threads": 12,
      "n_gpu_layers": 0
    },
    "advanced": {
      "path": "models/llama-2-70b-chat.Q4_K_M.gguf",
      "n_ctx": 8192,
      "n_threads": 16,
      "n_gpu_layers": 0
    }
  },
  "server": {
    "host": "0.0.0.0",
    "port": 8000,
    "max_concurrent_requests": 5
  }
}
```

### 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

–î–æ–±–∞–≤—å –≤ `.env`:

```env
# LLM –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
LLM_USE_LOCAL=true
LLM_LOCAL_URL=http://localhost:8000
LLM_FALLBACK_TO_OPENROUTER=true

# OpenRouter (–∫–∞–∫ fallback)
OPENROUTER_API_KEY=your_openrouter_key
```

---

## üöÄ –ó–∞–ø—É—Å–∫

### 1. –ó–∞–ø—É—Å–∫ LLM —Å–µ—Ä–≤–µ—Ä–∞

```bash
# –ó–∞–ø—É—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ LLM —Å–µ—Ä–≤–µ—Ä–∞
python src/llm/local_server.py
```

–°–µ—Ä–≤–µ—Ä –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: `http://localhost:8000`

### 2. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ—Ä–≤–µ—Ä–∞

```bash
# –¢–µ—Å—Ç –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–µ—Ä–∞
curl http://localhost:8000/health

# –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
    "context": "home",
    "model_type": "default"
  }'
```

### 3. –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –±–æ—Ç–∞

```bash
# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM
python run_enhanced_bot.py
```

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### 1. –ë–∞–∑–æ–≤—ã–µ —Ç–µ—Å—Ç—ã

```bash
# –¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
python test_local_llm_integration.py
```

### 2. –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
import asyncio
import time
from src.llm.client import LocalLLMClient

async def test_performance():
    async with LocalLLMClient() as client:
        start_time = time.time()
        
        response = await client.generate(
            prompt="–ù–∞–ø–∏—à–∏ —ç—Å—Å–µ –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ª–∏—á–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è",
            context="home"
        )
        
        end_time = time.time()
        print(f"–í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"–î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–¢–æ–∫–µ–Ω—ã: {response.tokens}")

asyncio.run(test_performance())
```

---

## üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### 1. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è CPU

```python
# –í llm_config.json
{
  "models": {
    "default": {
      "n_threads": 16,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU —è–¥–µ—Ä
      "n_batch": 512,   # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
      "n_ctx": 2048     # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    }
  }
}
```

### 2. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è GPU

```python
# –í llm_config.json
{
  "models": {
    "default": {
      "n_gpu_layers": 35,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –Ω–∞ GPU
      "n_batch": 1024,     # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –±–∞—Ç—á
      "n_ctx": 4096        # –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    }
  }
}
```

### 3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤

```bash
# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
htop

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
nvidia-smi
```

---

## üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ

### 1. –†–∞–±–æ—á–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

```python
# –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Ä–∞–±–æ—á–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
await client.set_session_context("session_123", "work")

response = await client.generate(
    prompt="–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞",
    context="work"
)
```

### 2. –î–æ–º–∞—à–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

```python
# –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–º–∞—à–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
await client.set_session_context("session_123", "home")

response = await client.generate(
    prompt="–õ–∏—á–Ω–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ",
    context="home"
)
```

### 3. –û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

```python
# –û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
response = await client.generate(
    prompt="–û–±—â–∏–µ —Å–æ–≤–µ—Ç—ã",
    context="general"
)
```

---

## üîç –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫

### 1. –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–∞–º—è—Ç—å—é

**–°–∏–º–ø—Ç–æ–º—ã**: –û—à–∏–±–∫–∏ "Out of memory"

**–†–µ—à–µ–Ω–∏—è**:
```python
# –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏
"n_ctx": 1024,  # –í–º–µ—Å—Ç–æ 4096
"n_batch": 256, # –í–º–µ—Å—Ç–æ 512

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ
# –°–∫–∞—á–∞—Ç—å Q2_K –≤–º–µ—Å—Ç–æ Q4_K_M
```

### 2. –ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

**–°–∏–º–ø—Ç–æ–º—ã**: –î–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞

**–†–µ—à–µ–Ω–∏—è**:
```python
# –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
"n_threads": 16,  # –í–º–µ—Å—Ç–æ 8

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU
"n_gpu_layers": 35,

# –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
"n_ctx": 2048,  # –í–º–µ—Å—Ç–æ 4096
```

### 3. –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏

**–°–∏–º–ø—Ç–æ–º—ã**: "Model not found"

**–†–µ—à–µ–Ω–∏—è**:
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
ls -la models/

# –ü–µ—Ä–µ—Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å
wget https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/resolve/main/llama-2-70b-chat.Q4_K_M.gguf
```

---

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

### 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llm_server.log'),
        logging.StreamHandler()
    ]
)
```

### 2. –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
# –í LLM —Å–µ—Ä–≤–∏—Å–µ
async def log_metrics(self, response: LLMResponse):
    logger.info(f"Generation metrics: {response.processing_time:.2f}s, "
                f"{response.tokens} tokens, confidence: {response.confidence:.2f}")
```

---

## üîÆ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

### 1. Fine-tuning

–ü–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏:

1. **–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö**: –≠–∫—Å–ø–æ—Ä—Ç –∏–∑ Notion
2. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞**: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
3. **LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ
4. **–í–∞–ª–∏–¥–∞—Ü–∏—è**: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞

### 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫

1. **–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è**: –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
2. **–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞**: ChromaDB –∏–ª–∏ FAISS
3. **–ü–æ–∏—Å–∫**: RAG (Retrieval-Augmented Generation)

### 3. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è

1. **–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –æ—Ç—á–µ—Ç—ã**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
2. **–£–º–Ω—ã–µ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è**: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
3. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è**: –¢—Ä–∏–≥–≥–µ—Ä—ã –∏ —Ä–µ–∞–∫—Ü–∏–∏

---

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [llama-cpp-python –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](https://github.com/abetlen/llama-cpp-python)
- [GGUF –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face](https://huggingface.co/TheBloke)
- [Llama 2 –º–æ–¥–µ–ª—å](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)
- [–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π](https://github.com/ggerganov/llama.cpp)

---

## üéØ –ß–µ–∫-–ª–∏—Å—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

- [ ] –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω llama-cpp-python
- [ ] –°–∫–∞—á–∞–Ω–∞ –º–æ–¥–µ–ª—å Llama 70B
- [ ] –ù–∞—Å—Ç—Ä–æ–µ–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
- [ ] –ó–∞–ø—É—â–µ–Ω LLM —Å–µ—Ä–≤–µ—Ä
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
- [ ] –ù–∞—Å—Ç—Ä–æ–µ–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ
- [ ] –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω —Å Telegram –±–æ—Ç–æ–º
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞

---

*–≠—Ç–æ—Ç –≥–∞–π–¥ –ø–æ–º–æ–∂–µ—Ç –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é LLM –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª—å–∑—ã –≤ —Ç–≤–æ–µ–π –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π AI-—ç–∫–æ—Å–∏—Å—Ç–µ–º–µ!* 