# Минимальные зависимости для LLM сервера
# Для реальной LLM интеграции добавь:
# llama-cpp-python==0.2.11
# transformers==4.35.0
# torch==2.1.0

# Базовые зависимости
requests==2.31.0
python-dotenv==1.0.0

# Для логирования
colorlog==6.7.0

# Для CORS (опционально)
flask-cors==4.0.0

# Для тестирования
pytest==7.4.3 