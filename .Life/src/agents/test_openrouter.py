import os
import asyncio
import httpx
from dotenv import load_dotenv

load_dotenv()

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

# –¢–µ—Å—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ OpenRouter
TEST_MODELS = {
    "GPT-3.5 Turbo": "openai/gpt-3.5-turbo",
    "Claude 3.5 Sonnet": "anthropic/claude-3.5-sonnet",
    "Llama 3.1 8B": "meta-llama/llama-3.1-8b-instruct",
    "Gemini Pro": "google/gemini-pro",
    "Mistral 7B": "mistralai/mistral-7b-instruct",
}

async def test_openrouter_model(model_name: str, model_id: str, test_prompt: str):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å OpenRouter"""
    print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
    print(f"ID –º–æ–¥–µ–ª–∏: {model_id}")
    
    url = "https://openrouter.ai/api/v1/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://github.com/notion-telegram-llm",
        "X-Title": "Notion-Telegram-LLM Integration"
    }
    
    payload = {
        "model": model_id,
        "messages": [
            {"role": "system", "content": "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."},
            {"role": "user", "content": test_prompt}
        ],
        "max_tokens": 200,
        "temperature": 0.7
    }
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, json=payload, timeout=30.0)
            response.raise_for_status()
            
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
            usage = result.get("usage", {})
            cost_info = ""
            if "total_tokens" in usage:
                cost_info = f" (—Ç–æ–∫–µ–Ω–æ–≤: {usage['total_tokens']})"
            
            print(f"‚úÖ –û—Ç–≤–µ—Ç: {content}")
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {cost_info}")
            
            return True, content, usage
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False, str(e), {}

async def test_all_models():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
    if not OPENROUTER_API_KEY:
        print("‚ùå OPENROUTER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
        return
    
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ OpenRouter API")
    print("=" * 50)
    
    test_prompt = "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞? –†–∞—Å—Å–∫–∞–∂–∏ –∫—Ä–∞—Ç–∫–æ –æ —Å–µ–±–µ."
    
    results = {}
    
    for model_name, model_id in TEST_MODELS.items():
        success, response, usage = await test_openrouter_model(model_name, model_id, test_prompt)
        results[model_name] = {
            "success": success,
            "response": response,
            "usage": usage
        }
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        await asyncio.sleep(1)
    
    # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\n" + "=" * 50)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 50)
    
    successful_models = 0
    total_tokens = 0
    
    for model_name, result in results.items():
        status = "‚úÖ" if result["success"] else "‚ùå"
        tokens = result["usage"].get("total_tokens", 0)
        total_tokens += tokens
        
        if result["success"]:
            successful_models += 1
        
        print(f"{status} {model_name}: {tokens} —Ç–æ–∫–µ–Ω–æ–≤")
    
    print(f"\nüéØ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ: {successful_models}/{len(TEST_MODELS)} –º–æ–¥–µ–ª–µ–π")
    print(f"üìà –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print("- GPT-3.5 Turbo: –¥–ª—è –æ–±—â–∏—Ö –∑–∞–¥–∞—á (–±—ã—Å—Ç—Ä–æ –∏ –¥—ë—à–µ–≤–æ)")
    print("- Claude 3.5 Sonnet: –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á")
    print("- Llama 3.1: –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤")
    print("- Gemini Pro: –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á")
    print("- Mistral 7B: –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏")

async def test_cost_optimization():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
    print("\nüí∞ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –°–¢–û–ò–ú–û–°–¢–ò")
    print("=" * 50)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
    model_id = "openai/gpt-3.5-turbo"
    
    test_configs = [
        {"max_tokens": 50, "temperature": 0.3, "name": "–ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç"},
        {"max_tokens": 100, "temperature": 0.7, "name": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç"},
        {"max_tokens": 200, "temperature": 1.0, "name": "–†–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç"},
    ]
    
    test_prompt = "–û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ API, –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏."
    
    for config in test_configs:
        print(f"\nüß™ {config['name']}")
        print(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏: max_tokens={config['max_tokens']}, temperature={config['temperature']}")
        
        url = "https://openrouter.ai/api/v1/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/notion-telegram-llm",
            "X-Title": "Notion-Telegram-LLM Integration"
        }
        
        payload = {
            "model": model_id,
            "messages": [
                {"role": "system", "content": "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."},
                {"role": "user", "content": test_prompt}
            ],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(url, headers=headers, json=payload, timeout=30.0)
                response.raise_for_status()
                
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                usage = result.get("usage", {})
                
                print(f"‚úÖ –û—Ç–≤–µ—Ç: {content}")
                print(f"üìä –¢–æ–∫–µ–Ω–æ–≤: {usage.get('total_tokens', 0)}")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    await test_all_models()
    await test_cost_optimization()

if __name__ == "__main__":
    asyncio.run(main()) 