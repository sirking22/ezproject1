# üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç: –õ–æ–∫–∞–ª—å–Ω–∞—è Llama 70B

## ‚ö° –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞ 15 –º–∏–Ω—É—Ç

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞–∫–µ—Ç—ã
pip install llama-cpp-python fastapi uvicorn

# –î–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
pip install sentence-transformers faiss-cpu

# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
pip install numpy pandas
```

### 2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
```bash
# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
mkdir -p models

# –°–∫–∞—á–∏–≤–∞–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é Llama 70B (4-bit, ~40GB)
wget https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/resolve/main/llama-2-70b-chat.Q4_K_M.gguf -O models/llama-2-70b-chat.Q4_K_M.gguf
```

### 3. –°–æ–∑–¥–∞–Ω–∏–µ API —Å–µ—Ä–≤–µ—Ä–∞
```python
# src/llm/quick_server.py
from fastapi import FastAPI
from llama_cpp import Llama
from pydantic import BaseModel

app = FastAPI()
llm = Llama(
    model_path="models/llama-2-70b-chat.Q4_K_M.gguf",
    n_ctx=4096,
    n_threads=8  # –ù–∞—Å—Ç—Ä–æ–π –ø–æ–¥ —Å–≤–æ—é —Å–∏—Å—Ç–µ–º—É
)

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 512

@app.post("/generate")
async def generate(request: GenerateRequest):
    response = llm(request.prompt, max_tokens=request.max_tokens)
    return {"response": response["choices"][0]["text"]}

@app.get("/health")
async def health():
    return {"status": "healthy"}
```

### 4. –ó–∞–ø—É—Å–∫
```bash
# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
uvicorn src.llm.quick_server:app --host 0.0.0.0 --port 8000

# –¢–µ—Å—Ç
curl -X POST "http://localhost:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"}'
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥ —Ç–≤–æ—é —Å–∏—Å—Ç–µ–º—É

### –î–ª—è –º–æ—â–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã (32+ GB RAM, 16+ —è–¥–µ—Ä):
```python
llm = Llama(
    model_path="models/llama-2-70b-chat.Q4_K_M.gguf",
    n_ctx=8192,
    n_threads=16,
    n_gpu_layers=35,  # –ï—Å–ª–∏ –µ—Å—Ç—å GPU
    n_batch=512
)
```

### –î–ª—è —Å—Ä–µ–¥–Ω–µ–π —Å–∏—Å—Ç–µ–º—ã (16+ GB RAM, 8+ —è–¥–µ—Ä):
```python
llm = Llama(
    model_path="models/llama-2-70b-chat.Q4_K_M.gguf",
    n_ctx=4096,
    n_threads=8,
    n_gpu_layers=0,
    n_batch=256
)
```

### –î–ª—è —Å–ª–∞–±–æ–π —Å–∏—Å—Ç–µ–º—ã (8+ GB RAM, 4+ —è–¥–µ—Ä):
```python
llm = Llama(
    model_path="models/llama-2-70b-chat.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0,
    n_batch=128
)
```

## üéØ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π

### –ó–∞–º–µ–Ω–∞ OpenRouter –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é LLM
```python
# –í src/agents/agent_core.py
import aiohttp

class AgentCore:
    def __init__(self):
        self.local_llm_url = "http://localhost:8000"
    
    async def get_agent_response(self, role: str, context: str, user_input: str) -> str:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.local_llm_url}/generate",
                json={"prompt": user_input, "max_tokens": 512}
            ) as response:
                result = await response.json()
                return result["response"]
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∏—Å—Ç–µ–º—ã
```bash
# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ RAM
htop

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
nvidia-smi

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã CPU
sensors
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
```python
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
CONFIGS = {
    "fast": {"n_threads": 16, "n_ctx": 2048, "n_batch": 512},
    "balanced": {"n_threads": 8, "n_ctx": 4096, "n_batch": 256},
    "memory_safe": {"n_threads": 4, "n_ctx": 1024, "n_batch": 128}
}
```

## üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

1. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π –±–∞–∑–æ–≤—É—é —Ä–∞–±–æ—Ç—É** - —É–±–µ–¥–∏—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç
2. **–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π —Å Telegram –±–æ—Ç–æ–º** - –∑–∞–º–µ–Ω–∏ OpenRouter
3. **–î–æ–±–∞–≤—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ** - —Ä–∞–±–æ—á–∏–π/–¥–æ–º–∞—à–Ω–∏–π —Ä–µ–∂–∏–º
4. **–ù–∞—Å—Ç—Ä–æ–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é** - –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã

- **RAM**: –ú–∏–Ω–∏–º—É–º 8GB, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 16+ GB
- **CPU**: –ú–∏–Ω–∏–º—É–º 4 —è–¥—Ä–∞, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 8+ —è–¥–µ—Ä
- **Storage**: 50+ GB –¥–ª—è –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö
- **GPU**: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É

## üéâ –ì–æ—Ç–æ–≤–æ!

–¢–µ–ø–µ—Ä—å —É —Ç–µ–±—è –µ—Å—Ç—å –ª–æ–∫–∞–ª—å–Ω–∞—è Llama 70B, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞ —Ç–≤–æ–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ. –ù–∏–∫–∞–∫–∏—Ö –≤–Ω–µ—à–Ω–∏—Ö API, –ø–æ–ª–Ω–∞—è –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –¥–∞–Ω–Ω—ã–º–∏. 