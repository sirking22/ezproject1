#!/usr/bin/env python3
"""
üß† –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –ö–û–ù–¢–ï–ö–°–¢–ê –ó–ê–î–ê–ß
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è —Å—Å—ã–ª–∫—É –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–¥–∞—á—É –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ–π
"""

import os
import logging
from typing import List, Dict, Optional, Any, Tuple
from dotenv import load_dotenv
from notion_client import AsyncClient
import asyncio
import re
from difflib import SequenceMatcher
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

class TaskAction(BaseModel):
    """–î–µ–π—Å—Ç–≤–∏–µ —Å –∑–∞–¥–∞—á–µ–π"""
    action_type: str = Field(description="–¢–∏–ø: add_subtask, update_time, mark_done, add_time")
    task_reference: str = Field(description="–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É (–Ω–∞–∑–≤–∞–Ω–∏–µ –∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)")
    subtask_name: Optional[str] = Field(default=None, description="–ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ–¥–∑–∞–¥–∞—á–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è")
    time_hours: Optional[float] = Field(default=None, description="–í—Ä–µ–º—è –≤ —á–∞—Å–∞—Ö")
    description: Optional[str] = Field(default=None, description="–û–ø–∏—Å–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è")
    confidence: float = Field(default=0.5, description="–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ (0-1)")

class ContextualAnalysis(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    actions: List[TaskAction]
    has_task_reference: bool = Field(description="–ï—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–¥–∞—á—É")
    extracted_context: str = Field(description="–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç")

class ContextualTaskAnalyzer:
    """üß† –ì–µ–Ω–∏–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–¥–∞—á"""
    
    def __init__(self):
        self.notion_token = os.getenv("NOTION_TOKEN")
        self.tasks_db_id = os.getenv("TASKS_DB")
        self.subtasks_db_id = os.getenv("SUBTASKS_DB")
        self.api_key = os.getenv("DEEPSEEK_API_KEY")
        self.base_url = os.getenv("DEEPSEEK_BASE_URL")
        
        if not all([self.notion_token, self.tasks_db_id, self.api_key, self.base_url]):
            raise ValueError("–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
            
        self.client = AsyncClient(auth=self.notion_token)
        self.llm = ChatOpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            model_name="deepseek-chat",
            temperature=0,
        )
        self.parser = JsonOutputParser(pydantic_object=ContextualAnalysis)
        
        # üß† –ì–ï–ù–ò–ê–õ–¨–ù–´–ô –ü–†–û–ú–ü–¢ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", "–¢—ã –≥–µ–Ω–∏–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–¥–∞—á. –¢–≤–æ—è —Ü–µ–ª—å - –ø–æ–Ω—è—Ç—å, –∫ –∫–∞–∫–æ–π –°–£–©–ï–°–¢–í–£–Æ–©–ï–ô –∑–∞–¥–∞—á–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."),
            ("system", "üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ê–í–ò–õ–ê –ê–ù–ê–õ–ò–ó–ê:"),
            ("system", "1. –ò—â–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–¥–∞—á–∏: '–∫ –ª–æ–≥–æ', '–≤ –æ–±–ª–æ–∂–∫–∞—Ö', '–¥–ª—è YouTube', '–ø–æ –∑–∞—Å—Ç–∞–≤–∫–µ'"),
            ("system", "2. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–µ–π—Å—Ç–≤–∏—è: add_subtask (–¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–∑–∞–¥–∞—á—É), update_time (–æ–±–Ω–æ–≤–∏—Ç—å –≤—Ä–µ–º—è), mark_done (–∑–∞–≤–µ—Ä—à–∏—Ç—å)"),
            ("system", "3. –ò–∑–≤–ª–µ–∫–∏ –≤—Ä–µ–º—è: '—á–∞—Å', '–¥–≤–∞ —á–∞—Å–∞', '–ø–æ–ª—Ç–æ—Ä–∞ —á–∞—Å–∞', '30 –º–∏–Ω—É—Ç' = 0.5"),
            ("system", "4. –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–¥–∞—á—É - has_task_reference = true"),
            ("system", "5. –û—Ü–µ–Ω–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ (0-1)"),
            ("system", "6. –ò—Å–ø–æ–ª—å–∑—É–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π"),
            ("system", "Format instructions: {format_instructions}"),
            ("human", "–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞—á: {existing_tasks_context}"),
            ("human", "–°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_message}"),
        ])
        
        self.chain = self.prompt.partial(format_instructions=self.parser.get_format_instructions()) | self.llm | self.parser
        
        logger.info("üß† ContextualTaskAnalyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def get_tasks_context(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞—á –¥–ª—è LLM"""
        try:
            logger.info("üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞—á...")
            
            response = await self.client.databases.query(
                database_id=self.tasks_db_id,
                page_size=50,  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 50 –∑–∞–¥–∞—á –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                sorts=[{"timestamp": "last_edited_time", "direction": "descending"}]
            )
            
            tasks_context = []
            for page in response.get("results", []):
                properties = page.get("properties", {})
                task_title = properties.get("–ó–∞–¥–∞—á–∞", {}).get("title", [])
                if task_title:
                    title = task_title[0]["text"]["content"]
                    status = properties.get("–°—Ç–∞—Ç—É—Å", {}).get("status", {}).get("name", "")
                    tasks_context.append(f"- {title} ({status})")
            
            context_text = "–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–¥–∞—á–∏:\n" + "\n".join(tasks_context[:20])  # –¢–æ–ø-20 –¥–ª—è LLM
            logger.info(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ {len(tasks_context)} –∑–∞–¥–∞—á")
            
            return context_text
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
            return "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
    
    def parse_llm_response(self, llm_response: Any) -> ContextualAnalysis:
        """üîß –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –æ—Ç–≤–µ—Ç–∞ LLM - –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç ContextualAnalysis"""
        try:
            # –ï—Å–ª–∏ —É–∂–µ –æ–±—ä–µ–∫—Ç ContextualAnalysis
            if isinstance(llm_response, ContextualAnalysis):
                return llm_response
            
            # –ï—Å–ª–∏ dict - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
            if isinstance(llm_response, dict):
                actions = []
                for action_data in llm_response.get("actions", []):
                    if isinstance(action_data, dict):
                        actions.append(TaskAction(**action_data))
                    else:
                        actions.append(action_data)
                
                return ContextualAnalysis(
                    actions=actions,
                    has_task_reference=llm_response.get("has_task_reference", False),
                    extracted_context=llm_response.get("extracted_context", "")
                )
            
            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ - –≤–æ–∑–º–æ–∂–Ω–æ JSON
            if isinstance(llm_response, str):
                try:
                    import json
                    data = json.loads(llm_response)
                    return self.parse_llm_response(data)  # –†–µ–∫—É—Ä—Å–∏—è –¥–ª—è dict
                except json.JSONDecodeError:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ —Å—Ç—Ä–æ–∫–∏: {llm_response[:100]}")
                    return ContextualAnalysis(
                        actions=[],
                        has_task_reference=False,
                        extracted_context=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {llm_response[:50]}..."
                    )
            
            # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø
            logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞ LLM: {type(llm_response)}")
            return ContextualAnalysis(
                actions=[],
                has_task_reference=False,
                extracted_context="–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞"
            )
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ—Ç–≤–µ—Ç–∞ LLM: {e}")
            return ContextualAnalysis(
                actions=[],
                has_task_reference=False,
                extracted_context=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}"
            )

    async def analyze_context(self, user_message: str) -> ContextualAnalysis:
        """üß† –ì–µ–Ω–∏–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        try:
            logger.info(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç: '{user_message}'")
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞—á
            tasks_context = await self.get_tasks_context()
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ LLM
            llm_response = self.chain.invoke({
                "existing_tasks_context": tasks_context,
                "user_message": user_message
            })
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
            analysis = self.parse_llm_response(llm_response)
            
            logger.info(f"üß† –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞: actions={len(analysis.actions)} has_task_reference={analysis.has_task_reference}")
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
            # –í–°–ï–ì–î–ê –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–π –æ–±—ä–µ–∫—Ç
            return ContextualAnalysis(
                actions=[],
                has_task_reference=False,
                extracted_context=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
            )
    
    async def find_target_task(self, task_reference: str) -> Optional[Dict[str, Any]]:
        """üéØ –ü–æ–∏—Å–∫ —Ü–µ–ª–µ–≤–æ–π –∑–∞–¥–∞—á–∏ –ø–æ —Å—Å—ã–ª–∫–µ"""
        try:
            logger.info(f"üéØ –ò—â–µ–º —Ü–µ–ª–µ–≤—É—é –∑–∞–¥–∞—á—É: '{task_reference}'")
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É)
            response = await self.client.databases.query(
                database_id=self.tasks_db_id,
                page_size=100  # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ –∑–∞–¥–∞—á –¥–ª—è –ø–æ–∏—Å–∫–∞
            )
            
            best_match = None
            best_score = 0.0
            
            for page in response.get("results", []):
                properties = page.get("properties", {})
                task_title = properties.get("–ó–∞–¥–∞—á–∞", {}).get("title", [])
                if task_title:
                    title = task_title[0]["text"]["content"]
                    
                    # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É)
                    if task_reference.lower() == title.lower():
                        logger.info(f"üéØ –¢–û–ß–ù–û–ï –°–û–í–ü–ê–î–ï–ù–ò–ï: '{title}'")
                        return {
                            "id": page["id"],
                            "title": title,
                            "status": properties.get("–°—Ç–∞—Ç—É—Å", {}).get("status", {}).get("name", ""),
                            "similarity": 1.0
                        }
                    
                    # 2. –°–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Å—Å—ã–ª–∫–∏
                    reference_words = set(re.findall(r'\b[–∞-—è—ë]{3,}\b', task_reference.lower()))
                    title_words = set(re.findall(r'\b[–∞-—è—ë]{3,}\b', title.lower()))
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
                    common_words = reference_words & title_words
                    if common_words:
                        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö —Å–ª–æ–≤
                        word_similarity = len(common_words) / max(len(reference_words), 1)
                        
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
                        string_similarity = SequenceMatcher(None, task_reference.lower(), title.lower()).ratio()
                        
                        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
                        combined_score = (word_similarity * 0.7) + (string_similarity * 0.3)
                        
                        if combined_score > best_score:
                            best_score = combined_score
                            best_match = {
                                "id": page["id"],
                                "title": title,
                                "status": properties.get("–°—Ç–∞—Ç—É—Å", {}).get("status", {}).get("name", ""),
                                "similarity": combined_score
                            }
            
            if best_match and best_score > 0.4:  # –ü–æ–≤—ã—à–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                logger.info(f"üéØ –ù–ê–ô–î–ï–ù–ê –¶–ï–õ–ï–í–ê–Ø –ó–ê–î–ê–ß–ê: '{best_match['title']}' (—Å—Ö–æ–¥—Å—Ç–≤–æ: {best_score:.2f})")
                return best_match
            
            logger.warning(f"‚ùå –¶–µ–ª–µ–≤–∞—è –∑–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è: '{task_reference}' (–ª—É—á—à–∏–π —Å–∫–æ—Ä: {best_score:.2f})")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ü–µ–ª–µ–≤–æ–π –∑–∞–¥–∞—á–∏: {e}")
            return None
    
    async def execute_action(self, action: TaskAction, target_task: Dict[str, Any], user_id: int) -> Dict[str, Any]:
        """‚ö° –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —Å –∑–∞–¥–∞—á–µ–π"""
        try:
            logger.info(f"‚ö° –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ: {action.action_type} –¥–ª—è '{target_task['title']}'")
            
            if action.action_type == "add_subtask":
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–∑–∞–¥–∞—á—É
                subtask_id = await self._add_subtask(target_task["id"], action)
                return {
                    "success": True,
                    "action": "subtask_added",
                    "subtask_name": action.subtask_name,
                    "subtask_id": subtask_id,
                    "time_hours": action.time_hours
                }
                
            elif action.action_type == "update_time":
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –∑–∞–¥–∞—á–∏
                await self._update_task_time(target_task["id"], action.time_hours)
                return {
                    "success": True,
                    "action": "time_updated",
                    "new_time": action.time_hours
                }
                
            elif action.action_type == "mark_done":
                # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
                await self._mark_task_done(target_task["id"])
                return {
                    "success": True,
                    "action": "marked_done"
                }
            
            return {"success": False, "error": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ"}
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –¥–µ–π—Å—Ç–≤–∏—è: {e}")
            return {"success": False, "error": str(e)}
    
    async def _add_subtask(self, parent_task_id: str, action: TaskAction) -> Optional[str]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–¥–∑–∞–¥–∞—á–∏"""
        try:
            properties = {
                "–ü–æ–¥–∑–∞–¥–∞—á–∏": {
                    "title": [{"text": {"content": action.subtask_name or "–ù–æ–≤–∞—è –ø–æ–¥–∑–∞–¥–∞—á–∞"}}]
                },
                "–°—Ç–∞—Ç—É—Å": {"status": {"name": "To Do"}},
                "–ó–∞–¥–∞—á–∏": {"relation": [{"id": parent_task_id}]}
            }
            
            if action.time_hours:
                properties["–í—Ä–µ–º—è"] = {"number": action.time_hours}
            
            if action.description:
                properties["–û–ø–∏—Å–∞–Ω–∏–µ"] = {
                    "rich_text": [{"text": {"content": action.description}}]
                }
            
            new_page = await self.client.pages.create(
                parent={"database_id": self.subtasks_db_id},
                properties=properties
            )
            
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∞: {action.subtask_name} ({action.time_hours} —á)")
            return new_page["id"]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –ø–æ–¥–∑–∞–¥–∞—á–∏: {e}")
            return None
    
    async def _update_task_time(self, task_id: str, time_hours: float):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–¥–∞—á–∏"""
        try:
            await self.client.pages.update(
                page_id=task_id,
                properties={"–ß–∞—Å—ã": {"number": time_hours}}
            )
            logger.info(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤—Ä–µ–º—è –∑–∞–¥–∞—á–∏: {time_hours} —á")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏: {e}")
    
    async def _mark_task_done(self, task_id: str):
        """–û—Ç–º–µ—Ç–∫–∞ –∑–∞–¥–∞—á–∏ –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π"""
        try:
            await self.client.pages.update(
                page_id=task_id,
                properties={"–°—Ç–∞—Ç—É—Å": {"status": {"name": "Done"}}}
            )
            logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ –æ—Ç–º–µ—á–µ–Ω–∞ –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–∞—è")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–º–µ—Ç–∫–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {e}")

# –¢–µ—Å—Ç –≥–µ–Ω–∏–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
async def test_contextual_analyzer():
    """–¢–µ—Å—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    analyzer = ContextualTaskAnalyzer()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    test_messages = [
        "–ö –ª–æ–≥–æ –¥–æ–±–∞–≤–∏—Ç—å —á–∞—Å –Ω–∞ –ø—Ä–∞–≤–∫–∏ —Å–∏–ª—É—ç—Ç–∞",
        "–í –æ–±–ª–æ–∂–∫–∞—Ö YouTube –ø–æ—Ç—Ä–∞—Ç–∏–ª –¥–≤–∞ —á–∞—Å–∞ –Ω–∞ —Ç–µ–∫—Å—Ç—ã",
        "–ó–∞—Å—Ç–∞–≤–∫–∞ –≥–æ—Ç–æ–≤–∞, –æ—Ç–º–µ—Ç–∏—Ç—å –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é",
        "–î–æ–±–∞–≤–∏—Ç—å –∫ –ª–æ–≥–æ—Ç–∏–ø—É –ø–æ–¥–∑–∞–¥–∞—á—É - —Ü–≤–µ—Ç–æ–≤—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã, –ø–æ–ª—Ç–æ—Ä–∞ —á–∞—Å–∞"
    ]
    
    for message in test_messages:
        print(f"\nüß† –¢–µ—Å—Ç: '{message}'")
        analysis = await analyzer.analyze_context(message)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {analysis}")

if __name__ == "__main__":
    asyncio.run(test_contextual_analyzer()) 